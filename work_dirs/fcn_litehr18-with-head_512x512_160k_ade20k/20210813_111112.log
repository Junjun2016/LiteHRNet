2021-08-13 11:11:12,863 - mmseg - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.7.6 (default, Jan  8 2020, 19:59:22) [GCC 7.3.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7: TITAN Xp
CUDA_HOME: /mnt/lustre/share/polaris/dep/cuda-9.0-cudnn7.6.5
NVCC: Cuda compilation tools, release 9.0, V9.0.176
GCC: gcc (GCC) 5.4.0
PyTorch: 1.5.0
PyTorch compiling details: PyTorch built with:
  - GCC 5.4
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.1 Product Build 20200208 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v0.21.1 (Git Hash 912ce228837d1ce28e1a61806118835de03f5751)
  - OpenMP 201307 (a.k.a. OpenMP 4.0)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 9.0
  - NVCC architecture flags: -gencode;arch=compute_52,code=sm_52;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_70,code=compute_70
  - CuDNN 7.6.5
  - Magma 2.5.0
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_INTERNAL_THREADPOOL_IMPL -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=ON, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

TorchVision: 0.6.0
OpenCV: 4.2.0
MMCV: 1.3.11
MMCV Compiler: n/a
MMCV CUDA Compiler: n/a
MMSegmentation: 0.16.0+2bb6f37
------------------------------------------------------------

2021-08-13 11:11:12,864 - mmseg - INFO - Distributed training: True
2021-08-13 11:11:13,224 - mmseg - INFO - Config:
norm_cfg = dict(type='SyncBN', requires_grad=True)
model = dict(
    type='EncoderDecoder',
    backbone=dict(
        type='LiteHRNet',
        norm_cfg=dict(type='SyncBN', requires_grad=True),
        norm_eval=False,
        extra=dict(
            stem=dict(stem_channels=32, out_channels=32, expand_ratio=1),
            num_stages=3,
            stages_spec=dict(
                num_modules=(2, 4, 2),
                num_branches=(2, 3, 4),
                num_blocks=(2, 2, 2),
                module_type=('LITE', 'LITE', 'LITE'),
                with_fuse=(True, True, True),
                reduce_ratios=(8, 8, 8),
                num_channels=((40, 80), (40, 80, 160), (40, 80, 160, 320))),
            with_head=True)),
    decode_head=dict(
        type='FCNHead',
        in_channels=40,
        in_index=0,
        channels=40,
        input_transform=None,
        kernel_size=3,
        num_convs=2,
        concat_input=True,
        dropout_ratio=0.1,
        num_classes=150,
        norm_cfg=dict(type='SyncBN', requires_grad=True),
        align_corners=False,
        loss_decode=dict(
            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0)),
    train_cfg=dict(),
    test_cfg=dict(mode='whole'))
dataset_type = 'ADE20KDataset'
data_root = 'data/ade/ADEChallengeData2016'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
crop_size = (512, 512)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', reduce_zero_label=True),
    dict(type='Resize', img_scale=(2048, 512), ratio_range=(0.5, 2.0)),
    dict(type='RandomCrop', crop_size=(512, 512), cat_max_ratio=0.75),
    dict(type='RandomFlip', prob=0.5),
    dict(type='PhotoMetricDistortion'),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_semantic_seg'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(2048, 512),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=4,
    workers_per_gpu=4,
    train=dict(
        type='ADE20KDataset',
        data_root='data/ade/ADEChallengeData2016',
        img_dir='images/training',
        ann_dir='annotations/training',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations', reduce_zero_label=True),
            dict(type='Resize', img_scale=(2048, 512), ratio_range=(0.5, 2.0)),
            dict(type='RandomCrop', crop_size=(512, 512), cat_max_ratio=0.75),
            dict(type='RandomFlip', prob=0.5),
            dict(type='PhotoMetricDistortion'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'gt_semantic_seg'])
        ]),
    val=dict(
        type='ADE20KDataset',
        data_root='data/ade/ADEChallengeData2016',
        img_dir='images/validation',
        ann_dir='annotations/validation',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(2048, 512),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='ADE20KDataset',
        data_root='data/ade/ADEChallengeData2016',
        img_dir='images/validation',
        ann_dir='annotations/validation',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(2048, 512),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
log_config = dict(
    interval=50, hooks=[dict(type='TextLoggerHook', by_epoch=False)])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
cudnn_benchmark = True
optimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0005)
optimizer_config = dict()
lr_config = dict(policy='poly', power=0.9, min_lr=0.0001, by_epoch=False)
runner = dict(type='IterBasedRunner', max_iters=160000)
checkpoint_config = dict(by_epoch=False, interval=16000)
evaluation = dict(interval=16000, metric='mIoU')
work_dir = './work_dirs/fcn_litehr18-with-head_512x512_160k_ade20k'
gpu_ids = range(0, 1)

2021-08-13 11:11:13,225 - mmseg - INFO - Set random seed to 0, deterministic: False
2021-08-13 11:11:13,422 - mmseg - INFO - initialize LiteHRNet with init_cfg [{'type': 'Normal', 'std': 0.001, 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
2021-08-13 11:11:13,742 - mmseg - INFO - initialize FCNHead with init_cfg {'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
Name of parameter - Initialization information

backbone.stem.conv1.conv.weight - torch.Size([32, 3, 3, 3]): 
Initialized by user-defined `init_weights` in ConvModule  

backbone.stem.conv1.bn.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stem.conv1.bn.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stem.branch1.0.conv.weight - torch.Size([16, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stem.branch1.0.bn.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stem.branch1.0.bn.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stem.branch1.1.conv.weight - torch.Size([16, 16, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stem.branch1.1.bn.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stem.branch1.1.bn.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stem.expand_conv.conv.weight - torch.Size([32, 16, 1, 1]): 
Initialized by user-defined `init_weights` in ConvModule  

backbone.stem.expand_conv.bn.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stem.expand_conv.bn.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stem.depthwise_conv.conv.weight - torch.Size([32, 1, 3, 3]): 
Initialized by user-defined `init_weights` in ConvModule  

backbone.stem.depthwise_conv.bn.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stem.depthwise_conv.bn.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stem.linear_conv.conv.weight - torch.Size([16, 32, 1, 1]): 
Initialized by user-defined `init_weights` in ConvModule  

backbone.stem.linear_conv.bn.weight - torch.Size([16]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stem.linear_conv.bn.bias - torch.Size([16]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.transition0.0.0.weight - torch.Size([32, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.transition0.0.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.transition0.0.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.transition0.0.2.weight - torch.Size([40, 32, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.transition0.0.3.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.transition0.0.3.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.transition0.1.0.0.weight - torch.Size([32, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.transition0.1.0.1.weight - torch.Size([32]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.transition0.1.0.1.bias - torch.Size([32]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.transition0.1.0.2.weight - torch.Size([80, 32, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.transition0.1.0.3.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.transition0.1.0.3.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.0.layers.0.cross_resolution_weighting.conv1.conv.weight - torch.Size([7, 60, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage0.0.layers.0.cross_resolution_weighting.conv1.bn.weight - torch.Size([7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.0.layers.0.cross_resolution_weighting.conv1.bn.bias - torch.Size([7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.0.layers.0.cross_resolution_weighting.conv2.conv.weight - torch.Size([60, 7, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage0.0.layers.0.cross_resolution_weighting.conv2.bn.weight - torch.Size([60]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.0.layers.0.cross_resolution_weighting.conv2.bn.bias - torch.Size([60]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.0.layers.0.depthwise_convs.0.conv.weight - torch.Size([20, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage0.0.layers.0.depthwise_convs.0.bn.weight - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.0.layers.0.depthwise_convs.0.bn.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.0.layers.0.depthwise_convs.1.conv.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage0.0.layers.0.depthwise_convs.1.bn.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.0.layers.0.depthwise_convs.1.bn.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.0.layers.0.spatial_weighting.0.conv1.conv.weight - torch.Size([5, 20, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage0.0.layers.0.spatial_weighting.0.conv1.conv.bias - torch.Size([5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.0.layers.0.spatial_weighting.0.conv2.conv.weight - torch.Size([20, 5, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage0.0.layers.0.spatial_weighting.0.conv2.conv.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.0.layers.0.spatial_weighting.1.conv1.conv.weight - torch.Size([10, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage0.0.layers.0.spatial_weighting.1.conv1.conv.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.0.layers.0.spatial_weighting.1.conv2.conv.weight - torch.Size([40, 10, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage0.0.layers.0.spatial_weighting.1.conv2.conv.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.0.layers.1.cross_resolution_weighting.conv1.conv.weight - torch.Size([7, 60, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage0.0.layers.1.cross_resolution_weighting.conv1.bn.weight - torch.Size([7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.0.layers.1.cross_resolution_weighting.conv1.bn.bias - torch.Size([7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.0.layers.1.cross_resolution_weighting.conv2.conv.weight - torch.Size([60, 7, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage0.0.layers.1.cross_resolution_weighting.conv2.bn.weight - torch.Size([60]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.0.layers.1.cross_resolution_weighting.conv2.bn.bias - torch.Size([60]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.0.layers.1.depthwise_convs.0.conv.weight - torch.Size([20, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage0.0.layers.1.depthwise_convs.0.bn.weight - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.0.layers.1.depthwise_convs.0.bn.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.0.layers.1.depthwise_convs.1.conv.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage0.0.layers.1.depthwise_convs.1.bn.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.0.layers.1.depthwise_convs.1.bn.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.0.layers.1.spatial_weighting.0.conv1.conv.weight - torch.Size([5, 20, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage0.0.layers.1.spatial_weighting.0.conv1.conv.bias - torch.Size([5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.0.layers.1.spatial_weighting.0.conv2.conv.weight - torch.Size([20, 5, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage0.0.layers.1.spatial_weighting.0.conv2.conv.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.0.layers.1.spatial_weighting.1.conv1.conv.weight - torch.Size([10, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage0.0.layers.1.spatial_weighting.1.conv1.conv.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.0.layers.1.spatial_weighting.1.conv2.conv.weight - torch.Size([40, 10, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage0.0.layers.1.spatial_weighting.1.conv2.conv.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.0.fuse_layers.0.1.0.weight - torch.Size([40, 80, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage0.0.fuse_layers.0.1.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.0.fuse_layers.0.1.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.0.fuse_layers.1.0.0.0.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage0.0.fuse_layers.1.0.0.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.0.fuse_layers.1.0.0.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.0.fuse_layers.1.0.0.2.weight - torch.Size([80, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage0.0.fuse_layers.1.0.0.3.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.0.fuse_layers.1.0.0.3.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.1.layers.0.cross_resolution_weighting.conv1.conv.weight - torch.Size([7, 60, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage0.1.layers.0.cross_resolution_weighting.conv1.bn.weight - torch.Size([7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.1.layers.0.cross_resolution_weighting.conv1.bn.bias - torch.Size([7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.1.layers.0.cross_resolution_weighting.conv2.conv.weight - torch.Size([60, 7, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage0.1.layers.0.cross_resolution_weighting.conv2.bn.weight - torch.Size([60]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.1.layers.0.cross_resolution_weighting.conv2.bn.bias - torch.Size([60]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.1.layers.0.depthwise_convs.0.conv.weight - torch.Size([20, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage0.1.layers.0.depthwise_convs.0.bn.weight - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.1.layers.0.depthwise_convs.0.bn.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.1.layers.0.depthwise_convs.1.conv.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage0.1.layers.0.depthwise_convs.1.bn.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.1.layers.0.depthwise_convs.1.bn.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.1.layers.0.spatial_weighting.0.conv1.conv.weight - torch.Size([5, 20, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage0.1.layers.0.spatial_weighting.0.conv1.conv.bias - torch.Size([5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.1.layers.0.spatial_weighting.0.conv2.conv.weight - torch.Size([20, 5, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage0.1.layers.0.spatial_weighting.0.conv2.conv.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.1.layers.0.spatial_weighting.1.conv1.conv.weight - torch.Size([10, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage0.1.layers.0.spatial_weighting.1.conv1.conv.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.1.layers.0.spatial_weighting.1.conv2.conv.weight - torch.Size([40, 10, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage0.1.layers.0.spatial_weighting.1.conv2.conv.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.1.layers.1.cross_resolution_weighting.conv1.conv.weight - torch.Size([7, 60, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage0.1.layers.1.cross_resolution_weighting.conv1.bn.weight - torch.Size([7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.1.layers.1.cross_resolution_weighting.conv1.bn.bias - torch.Size([7]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.1.layers.1.cross_resolution_weighting.conv2.conv.weight - torch.Size([60, 7, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage0.1.layers.1.cross_resolution_weighting.conv2.bn.weight - torch.Size([60]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.1.layers.1.cross_resolution_weighting.conv2.bn.bias - torch.Size([60]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.1.layers.1.depthwise_convs.0.conv.weight - torch.Size([20, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage0.1.layers.1.depthwise_convs.0.bn.weight - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.1.layers.1.depthwise_convs.0.bn.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.1.layers.1.depthwise_convs.1.conv.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage0.1.layers.1.depthwise_convs.1.bn.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.1.layers.1.depthwise_convs.1.bn.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.1.layers.1.spatial_weighting.0.conv1.conv.weight - torch.Size([5, 20, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage0.1.layers.1.spatial_weighting.0.conv1.conv.bias - torch.Size([5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.1.layers.1.spatial_weighting.0.conv2.conv.weight - torch.Size([20, 5, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage0.1.layers.1.spatial_weighting.0.conv2.conv.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.1.layers.1.spatial_weighting.1.conv1.conv.weight - torch.Size([10, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage0.1.layers.1.spatial_weighting.1.conv1.conv.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.1.layers.1.spatial_weighting.1.conv2.conv.weight - torch.Size([40, 10, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage0.1.layers.1.spatial_weighting.1.conv2.conv.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.1.fuse_layers.0.1.0.weight - torch.Size([40, 80, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage0.1.fuse_layers.0.1.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.1.fuse_layers.0.1.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.1.fuse_layers.1.0.0.0.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage0.1.fuse_layers.1.0.0.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.1.fuse_layers.1.0.0.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.1.fuse_layers.1.0.0.2.weight - torch.Size([80, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage0.1.fuse_layers.1.0.0.3.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage0.1.fuse_layers.1.0.0.3.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.transition1.2.0.0.weight - torch.Size([80, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.transition1.2.0.1.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.transition1.2.0.1.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.transition1.2.0.2.weight - torch.Size([160, 80, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.transition1.2.0.3.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.transition1.2.0.3.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.layers.0.cross_resolution_weighting.conv1.conv.weight - torch.Size([17, 140, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.0.layers.0.cross_resolution_weighting.conv1.bn.weight - torch.Size([17]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.layers.0.cross_resolution_weighting.conv1.bn.bias - torch.Size([17]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.layers.0.cross_resolution_weighting.conv2.conv.weight - torch.Size([140, 17, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.0.layers.0.cross_resolution_weighting.conv2.bn.weight - torch.Size([140]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.layers.0.cross_resolution_weighting.conv2.bn.bias - torch.Size([140]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.layers.0.depthwise_convs.0.conv.weight - torch.Size([20, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.0.layers.0.depthwise_convs.0.bn.weight - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.layers.0.depthwise_convs.0.bn.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.layers.0.depthwise_convs.1.conv.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.0.layers.0.depthwise_convs.1.bn.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.layers.0.depthwise_convs.1.bn.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.layers.0.depthwise_convs.2.conv.weight - torch.Size([80, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.0.layers.0.depthwise_convs.2.bn.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.layers.0.depthwise_convs.2.bn.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.layers.0.spatial_weighting.0.conv1.conv.weight - torch.Size([5, 20, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.0.layers.0.spatial_weighting.0.conv1.conv.bias - torch.Size([5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.layers.0.spatial_weighting.0.conv2.conv.weight - torch.Size([20, 5, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.0.layers.0.spatial_weighting.0.conv2.conv.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.layers.0.spatial_weighting.1.conv1.conv.weight - torch.Size([10, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.0.layers.0.spatial_weighting.1.conv1.conv.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.layers.0.spatial_weighting.1.conv2.conv.weight - torch.Size([40, 10, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.0.layers.0.spatial_weighting.1.conv2.conv.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.layers.0.spatial_weighting.2.conv1.conv.weight - torch.Size([20, 80, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.0.layers.0.spatial_weighting.2.conv1.conv.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.layers.0.spatial_weighting.2.conv2.conv.weight - torch.Size([80, 20, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.0.layers.0.spatial_weighting.2.conv2.conv.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.layers.1.cross_resolution_weighting.conv1.conv.weight - torch.Size([17, 140, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.0.layers.1.cross_resolution_weighting.conv1.bn.weight - torch.Size([17]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.layers.1.cross_resolution_weighting.conv1.bn.bias - torch.Size([17]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.layers.1.cross_resolution_weighting.conv2.conv.weight - torch.Size([140, 17, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.0.layers.1.cross_resolution_weighting.conv2.bn.weight - torch.Size([140]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.layers.1.cross_resolution_weighting.conv2.bn.bias - torch.Size([140]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.layers.1.depthwise_convs.0.conv.weight - torch.Size([20, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.0.layers.1.depthwise_convs.0.bn.weight - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.layers.1.depthwise_convs.0.bn.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.layers.1.depthwise_convs.1.conv.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.0.layers.1.depthwise_convs.1.bn.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.layers.1.depthwise_convs.1.bn.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.layers.1.depthwise_convs.2.conv.weight - torch.Size([80, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.0.layers.1.depthwise_convs.2.bn.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.layers.1.depthwise_convs.2.bn.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.layers.1.spatial_weighting.0.conv1.conv.weight - torch.Size([5, 20, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.0.layers.1.spatial_weighting.0.conv1.conv.bias - torch.Size([5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.layers.1.spatial_weighting.0.conv2.conv.weight - torch.Size([20, 5, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.0.layers.1.spatial_weighting.0.conv2.conv.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.layers.1.spatial_weighting.1.conv1.conv.weight - torch.Size([10, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.0.layers.1.spatial_weighting.1.conv1.conv.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.layers.1.spatial_weighting.1.conv2.conv.weight - torch.Size([40, 10, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.0.layers.1.spatial_weighting.1.conv2.conv.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.layers.1.spatial_weighting.2.conv1.conv.weight - torch.Size([20, 80, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.0.layers.1.spatial_weighting.2.conv1.conv.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.layers.1.spatial_weighting.2.conv2.conv.weight - torch.Size([80, 20, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.0.layers.1.spatial_weighting.2.conv2.conv.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.fuse_layers.0.1.0.weight - torch.Size([40, 80, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.0.fuse_layers.0.1.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.fuse_layers.0.1.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.fuse_layers.0.2.0.weight - torch.Size([40, 160, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.0.fuse_layers.0.2.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.fuse_layers.0.2.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.fuse_layers.1.0.0.0.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.0.fuse_layers.1.0.0.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.fuse_layers.1.0.0.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.fuse_layers.1.0.0.2.weight - torch.Size([80, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.0.fuse_layers.1.0.0.3.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.fuse_layers.1.0.0.3.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.fuse_layers.1.2.0.weight - torch.Size([80, 160, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.0.fuse_layers.1.2.1.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.fuse_layers.1.2.1.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.fuse_layers.2.0.0.0.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.0.fuse_layers.2.0.0.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.fuse_layers.2.0.0.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.fuse_layers.2.0.0.2.weight - torch.Size([40, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.0.fuse_layers.2.0.0.3.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.fuse_layers.2.0.0.3.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.fuse_layers.2.0.1.0.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.0.fuse_layers.2.0.1.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.fuse_layers.2.0.1.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.fuse_layers.2.0.1.2.weight - torch.Size([160, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.0.fuse_layers.2.0.1.3.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.fuse_layers.2.0.1.3.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.fuse_layers.2.1.0.0.weight - torch.Size([80, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.0.fuse_layers.2.1.0.1.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.fuse_layers.2.1.0.1.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.fuse_layers.2.1.0.2.weight - torch.Size([160, 80, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.0.fuse_layers.2.1.0.3.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.0.fuse_layers.2.1.0.3.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.layers.0.cross_resolution_weighting.conv1.conv.weight - torch.Size([17, 140, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.1.layers.0.cross_resolution_weighting.conv1.bn.weight - torch.Size([17]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.layers.0.cross_resolution_weighting.conv1.bn.bias - torch.Size([17]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.layers.0.cross_resolution_weighting.conv2.conv.weight - torch.Size([140, 17, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.1.layers.0.cross_resolution_weighting.conv2.bn.weight - torch.Size([140]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.layers.0.cross_resolution_weighting.conv2.bn.bias - torch.Size([140]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.layers.0.depthwise_convs.0.conv.weight - torch.Size([20, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.1.layers.0.depthwise_convs.0.bn.weight - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.layers.0.depthwise_convs.0.bn.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.layers.0.depthwise_convs.1.conv.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.1.layers.0.depthwise_convs.1.bn.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.layers.0.depthwise_convs.1.bn.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.layers.0.depthwise_convs.2.conv.weight - torch.Size([80, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.1.layers.0.depthwise_convs.2.bn.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.layers.0.depthwise_convs.2.bn.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.layers.0.spatial_weighting.0.conv1.conv.weight - torch.Size([5, 20, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.1.layers.0.spatial_weighting.0.conv1.conv.bias - torch.Size([5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.layers.0.spatial_weighting.0.conv2.conv.weight - torch.Size([20, 5, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.1.layers.0.spatial_weighting.0.conv2.conv.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.layers.0.spatial_weighting.1.conv1.conv.weight - torch.Size([10, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.1.layers.0.spatial_weighting.1.conv1.conv.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.layers.0.spatial_weighting.1.conv2.conv.weight - torch.Size([40, 10, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.1.layers.0.spatial_weighting.1.conv2.conv.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.layers.0.spatial_weighting.2.conv1.conv.weight - torch.Size([20, 80, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.1.layers.0.spatial_weighting.2.conv1.conv.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.layers.0.spatial_weighting.2.conv2.conv.weight - torch.Size([80, 20, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.1.layers.0.spatial_weighting.2.conv2.conv.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.layers.1.cross_resolution_weighting.conv1.conv.weight - torch.Size([17, 140, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.1.layers.1.cross_resolution_weighting.conv1.bn.weight - torch.Size([17]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.layers.1.cross_resolution_weighting.conv1.bn.bias - torch.Size([17]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.layers.1.cross_resolution_weighting.conv2.conv.weight - torch.Size([140, 17, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.1.layers.1.cross_resolution_weighting.conv2.bn.weight - torch.Size([140]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.layers.1.cross_resolution_weighting.conv2.bn.bias - torch.Size([140]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.layers.1.depthwise_convs.0.conv.weight - torch.Size([20, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.1.layers.1.depthwise_convs.0.bn.weight - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.layers.1.depthwise_convs.0.bn.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.layers.1.depthwise_convs.1.conv.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.1.layers.1.depthwise_convs.1.bn.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.layers.1.depthwise_convs.1.bn.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.layers.1.depthwise_convs.2.conv.weight - torch.Size([80, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.1.layers.1.depthwise_convs.2.bn.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.layers.1.depthwise_convs.2.bn.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.layers.1.spatial_weighting.0.conv1.conv.weight - torch.Size([5, 20, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.1.layers.1.spatial_weighting.0.conv1.conv.bias - torch.Size([5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.layers.1.spatial_weighting.0.conv2.conv.weight - torch.Size([20, 5, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.1.layers.1.spatial_weighting.0.conv2.conv.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.layers.1.spatial_weighting.1.conv1.conv.weight - torch.Size([10, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.1.layers.1.spatial_weighting.1.conv1.conv.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.layers.1.spatial_weighting.1.conv2.conv.weight - torch.Size([40, 10, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.1.layers.1.spatial_weighting.1.conv2.conv.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.layers.1.spatial_weighting.2.conv1.conv.weight - torch.Size([20, 80, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.1.layers.1.spatial_weighting.2.conv1.conv.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.layers.1.spatial_weighting.2.conv2.conv.weight - torch.Size([80, 20, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.1.layers.1.spatial_weighting.2.conv2.conv.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.fuse_layers.0.1.0.weight - torch.Size([40, 80, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.1.fuse_layers.0.1.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.fuse_layers.0.1.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.fuse_layers.0.2.0.weight - torch.Size([40, 160, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.1.fuse_layers.0.2.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.fuse_layers.0.2.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.fuse_layers.1.0.0.0.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.1.fuse_layers.1.0.0.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.fuse_layers.1.0.0.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.fuse_layers.1.0.0.2.weight - torch.Size([80, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.1.fuse_layers.1.0.0.3.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.fuse_layers.1.0.0.3.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.fuse_layers.1.2.0.weight - torch.Size([80, 160, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.1.fuse_layers.1.2.1.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.fuse_layers.1.2.1.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.fuse_layers.2.0.0.0.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.1.fuse_layers.2.0.0.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.fuse_layers.2.0.0.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.fuse_layers.2.0.0.2.weight - torch.Size([40, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.1.fuse_layers.2.0.0.3.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.fuse_layers.2.0.0.3.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.fuse_layers.2.0.1.0.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.1.fuse_layers.2.0.1.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.fuse_layers.2.0.1.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.fuse_layers.2.0.1.2.weight - torch.Size([160, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.1.fuse_layers.2.0.1.3.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.fuse_layers.2.0.1.3.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.fuse_layers.2.1.0.0.weight - torch.Size([80, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.1.fuse_layers.2.1.0.1.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.fuse_layers.2.1.0.1.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.fuse_layers.2.1.0.2.weight - torch.Size([160, 80, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.1.fuse_layers.2.1.0.3.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.1.fuse_layers.2.1.0.3.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.layers.0.cross_resolution_weighting.conv1.conv.weight - torch.Size([17, 140, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.2.layers.0.cross_resolution_weighting.conv1.bn.weight - torch.Size([17]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.layers.0.cross_resolution_weighting.conv1.bn.bias - torch.Size([17]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.layers.0.cross_resolution_weighting.conv2.conv.weight - torch.Size([140, 17, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.2.layers.0.cross_resolution_weighting.conv2.bn.weight - torch.Size([140]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.layers.0.cross_resolution_weighting.conv2.bn.bias - torch.Size([140]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.layers.0.depthwise_convs.0.conv.weight - torch.Size([20, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.2.layers.0.depthwise_convs.0.bn.weight - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.layers.0.depthwise_convs.0.bn.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.layers.0.depthwise_convs.1.conv.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.2.layers.0.depthwise_convs.1.bn.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.layers.0.depthwise_convs.1.bn.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.layers.0.depthwise_convs.2.conv.weight - torch.Size([80, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.2.layers.0.depthwise_convs.2.bn.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.layers.0.depthwise_convs.2.bn.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.layers.0.spatial_weighting.0.conv1.conv.weight - torch.Size([5, 20, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.2.layers.0.spatial_weighting.0.conv1.conv.bias - torch.Size([5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.layers.0.spatial_weighting.0.conv2.conv.weight - torch.Size([20, 5, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.2.layers.0.spatial_weighting.0.conv2.conv.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.layers.0.spatial_weighting.1.conv1.conv.weight - torch.Size([10, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.2.layers.0.spatial_weighting.1.conv1.conv.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.layers.0.spatial_weighting.1.conv2.conv.weight - torch.Size([40, 10, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.2.layers.0.spatial_weighting.1.conv2.conv.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.layers.0.spatial_weighting.2.conv1.conv.weight - torch.Size([20, 80, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.2.layers.0.spatial_weighting.2.conv1.conv.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.layers.0.spatial_weighting.2.conv2.conv.weight - torch.Size([80, 20, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.2.layers.0.spatial_weighting.2.conv2.conv.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.layers.1.cross_resolution_weighting.conv1.conv.weight - torch.Size([17, 140, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.2.layers.1.cross_resolution_weighting.conv1.bn.weight - torch.Size([17]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.layers.1.cross_resolution_weighting.conv1.bn.bias - torch.Size([17]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.layers.1.cross_resolution_weighting.conv2.conv.weight - torch.Size([140, 17, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.2.layers.1.cross_resolution_weighting.conv2.bn.weight - torch.Size([140]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.layers.1.cross_resolution_weighting.conv2.bn.bias - torch.Size([140]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.layers.1.depthwise_convs.0.conv.weight - torch.Size([20, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.2.layers.1.depthwise_convs.0.bn.weight - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.layers.1.depthwise_convs.0.bn.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.layers.1.depthwise_convs.1.conv.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.2.layers.1.depthwise_convs.1.bn.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.layers.1.depthwise_convs.1.bn.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.layers.1.depthwise_convs.2.conv.weight - torch.Size([80, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.2.layers.1.depthwise_convs.2.bn.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.layers.1.depthwise_convs.2.bn.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.layers.1.spatial_weighting.0.conv1.conv.weight - torch.Size([5, 20, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.2.layers.1.spatial_weighting.0.conv1.conv.bias - torch.Size([5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.layers.1.spatial_weighting.0.conv2.conv.weight - torch.Size([20, 5, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.2.layers.1.spatial_weighting.0.conv2.conv.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.layers.1.spatial_weighting.1.conv1.conv.weight - torch.Size([10, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.2.layers.1.spatial_weighting.1.conv1.conv.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.layers.1.spatial_weighting.1.conv2.conv.weight - torch.Size([40, 10, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.2.layers.1.spatial_weighting.1.conv2.conv.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.layers.1.spatial_weighting.2.conv1.conv.weight - torch.Size([20, 80, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.2.layers.1.spatial_weighting.2.conv1.conv.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.layers.1.spatial_weighting.2.conv2.conv.weight - torch.Size([80, 20, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.2.layers.1.spatial_weighting.2.conv2.conv.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.fuse_layers.0.1.0.weight - torch.Size([40, 80, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.2.fuse_layers.0.1.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.fuse_layers.0.1.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.fuse_layers.0.2.0.weight - torch.Size([40, 160, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.2.fuse_layers.0.2.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.fuse_layers.0.2.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.fuse_layers.1.0.0.0.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.2.fuse_layers.1.0.0.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.fuse_layers.1.0.0.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.fuse_layers.1.0.0.2.weight - torch.Size([80, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.2.fuse_layers.1.0.0.3.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.fuse_layers.1.0.0.3.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.fuse_layers.1.2.0.weight - torch.Size([80, 160, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.2.fuse_layers.1.2.1.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.fuse_layers.1.2.1.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.fuse_layers.2.0.0.0.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.2.fuse_layers.2.0.0.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.fuse_layers.2.0.0.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.fuse_layers.2.0.0.2.weight - torch.Size([40, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.2.fuse_layers.2.0.0.3.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.fuse_layers.2.0.0.3.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.fuse_layers.2.0.1.0.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.2.fuse_layers.2.0.1.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.fuse_layers.2.0.1.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.fuse_layers.2.0.1.2.weight - torch.Size([160, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.2.fuse_layers.2.0.1.3.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.fuse_layers.2.0.1.3.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.fuse_layers.2.1.0.0.weight - torch.Size([80, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.2.fuse_layers.2.1.0.1.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.fuse_layers.2.1.0.1.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.fuse_layers.2.1.0.2.weight - torch.Size([160, 80, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.2.fuse_layers.2.1.0.3.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.2.fuse_layers.2.1.0.3.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.layers.0.cross_resolution_weighting.conv1.conv.weight - torch.Size([17, 140, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.3.layers.0.cross_resolution_weighting.conv1.bn.weight - torch.Size([17]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.layers.0.cross_resolution_weighting.conv1.bn.bias - torch.Size([17]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.layers.0.cross_resolution_weighting.conv2.conv.weight - torch.Size([140, 17, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.3.layers.0.cross_resolution_weighting.conv2.bn.weight - torch.Size([140]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.layers.0.cross_resolution_weighting.conv2.bn.bias - torch.Size([140]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.layers.0.depthwise_convs.0.conv.weight - torch.Size([20, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.3.layers.0.depthwise_convs.0.bn.weight - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.layers.0.depthwise_convs.0.bn.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.layers.0.depthwise_convs.1.conv.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.3.layers.0.depthwise_convs.1.bn.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.layers.0.depthwise_convs.1.bn.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.layers.0.depthwise_convs.2.conv.weight - torch.Size([80, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.3.layers.0.depthwise_convs.2.bn.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.layers.0.depthwise_convs.2.bn.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.layers.0.spatial_weighting.0.conv1.conv.weight - torch.Size([5, 20, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.3.layers.0.spatial_weighting.0.conv1.conv.bias - torch.Size([5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.layers.0.spatial_weighting.0.conv2.conv.weight - torch.Size([20, 5, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.3.layers.0.spatial_weighting.0.conv2.conv.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.layers.0.spatial_weighting.1.conv1.conv.weight - torch.Size([10, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.3.layers.0.spatial_weighting.1.conv1.conv.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.layers.0.spatial_weighting.1.conv2.conv.weight - torch.Size([40, 10, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.3.layers.0.spatial_weighting.1.conv2.conv.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.layers.0.spatial_weighting.2.conv1.conv.weight - torch.Size([20, 80, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.3.layers.0.spatial_weighting.2.conv1.conv.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.layers.0.spatial_weighting.2.conv2.conv.weight - torch.Size([80, 20, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.3.layers.0.spatial_weighting.2.conv2.conv.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.layers.1.cross_resolution_weighting.conv1.conv.weight - torch.Size([17, 140, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.3.layers.1.cross_resolution_weighting.conv1.bn.weight - torch.Size([17]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.layers.1.cross_resolution_weighting.conv1.bn.bias - torch.Size([17]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.layers.1.cross_resolution_weighting.conv2.conv.weight - torch.Size([140, 17, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.3.layers.1.cross_resolution_weighting.conv2.bn.weight - torch.Size([140]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.layers.1.cross_resolution_weighting.conv2.bn.bias - torch.Size([140]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.layers.1.depthwise_convs.0.conv.weight - torch.Size([20, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.3.layers.1.depthwise_convs.0.bn.weight - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.layers.1.depthwise_convs.0.bn.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.layers.1.depthwise_convs.1.conv.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.3.layers.1.depthwise_convs.1.bn.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.layers.1.depthwise_convs.1.bn.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.layers.1.depthwise_convs.2.conv.weight - torch.Size([80, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.3.layers.1.depthwise_convs.2.bn.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.layers.1.depthwise_convs.2.bn.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.layers.1.spatial_weighting.0.conv1.conv.weight - torch.Size([5, 20, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.3.layers.1.spatial_weighting.0.conv1.conv.bias - torch.Size([5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.layers.1.spatial_weighting.0.conv2.conv.weight - torch.Size([20, 5, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.3.layers.1.spatial_weighting.0.conv2.conv.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.layers.1.spatial_weighting.1.conv1.conv.weight - torch.Size([10, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.3.layers.1.spatial_weighting.1.conv1.conv.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.layers.1.spatial_weighting.1.conv2.conv.weight - torch.Size([40, 10, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.3.layers.1.spatial_weighting.1.conv2.conv.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.layers.1.spatial_weighting.2.conv1.conv.weight - torch.Size([20, 80, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.3.layers.1.spatial_weighting.2.conv1.conv.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.layers.1.spatial_weighting.2.conv2.conv.weight - torch.Size([80, 20, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.3.layers.1.spatial_weighting.2.conv2.conv.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.fuse_layers.0.1.0.weight - torch.Size([40, 80, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.3.fuse_layers.0.1.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.fuse_layers.0.1.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.fuse_layers.0.2.0.weight - torch.Size([40, 160, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.3.fuse_layers.0.2.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.fuse_layers.0.2.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.fuse_layers.1.0.0.0.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.3.fuse_layers.1.0.0.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.fuse_layers.1.0.0.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.fuse_layers.1.0.0.2.weight - torch.Size([80, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.3.fuse_layers.1.0.0.3.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.fuse_layers.1.0.0.3.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.fuse_layers.1.2.0.weight - torch.Size([80, 160, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.3.fuse_layers.1.2.1.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.fuse_layers.1.2.1.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.fuse_layers.2.0.0.0.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.3.fuse_layers.2.0.0.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.fuse_layers.2.0.0.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.fuse_layers.2.0.0.2.weight - torch.Size([40, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.3.fuse_layers.2.0.0.3.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.fuse_layers.2.0.0.3.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.fuse_layers.2.0.1.0.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.3.fuse_layers.2.0.1.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.fuse_layers.2.0.1.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.fuse_layers.2.0.1.2.weight - torch.Size([160, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.3.fuse_layers.2.0.1.3.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.fuse_layers.2.0.1.3.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.fuse_layers.2.1.0.0.weight - torch.Size([80, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.3.fuse_layers.2.1.0.1.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.fuse_layers.2.1.0.1.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.fuse_layers.2.1.0.2.weight - torch.Size([160, 80, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage1.3.fuse_layers.2.1.0.3.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage1.3.fuse_layers.2.1.0.3.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.transition2.3.0.0.weight - torch.Size([160, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.transition2.3.0.1.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.transition2.3.0.1.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.transition2.3.0.2.weight - torch.Size([320, 160, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.transition2.3.0.3.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.transition2.3.0.3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.layers.0.cross_resolution_weighting.conv1.conv.weight - torch.Size([37, 300, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.layers.0.cross_resolution_weighting.conv1.bn.weight - torch.Size([37]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.layers.0.cross_resolution_weighting.conv1.bn.bias - torch.Size([37]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.layers.0.cross_resolution_weighting.conv2.conv.weight - torch.Size([300, 37, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.layers.0.cross_resolution_weighting.conv2.bn.weight - torch.Size([300]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.layers.0.cross_resolution_weighting.conv2.bn.bias - torch.Size([300]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.layers.0.depthwise_convs.0.conv.weight - torch.Size([20, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.layers.0.depthwise_convs.0.bn.weight - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.layers.0.depthwise_convs.0.bn.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.layers.0.depthwise_convs.1.conv.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.layers.0.depthwise_convs.1.bn.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.layers.0.depthwise_convs.1.bn.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.layers.0.depthwise_convs.2.conv.weight - torch.Size([80, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.layers.0.depthwise_convs.2.bn.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.layers.0.depthwise_convs.2.bn.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.layers.0.depthwise_convs.3.conv.weight - torch.Size([160, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.layers.0.depthwise_convs.3.bn.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.layers.0.depthwise_convs.3.bn.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.layers.0.spatial_weighting.0.conv1.conv.weight - torch.Size([5, 20, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.layers.0.spatial_weighting.0.conv1.conv.bias - torch.Size([5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.layers.0.spatial_weighting.0.conv2.conv.weight - torch.Size([20, 5, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.layers.0.spatial_weighting.0.conv2.conv.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.layers.0.spatial_weighting.1.conv1.conv.weight - torch.Size([10, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.layers.0.spatial_weighting.1.conv1.conv.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.layers.0.spatial_weighting.1.conv2.conv.weight - torch.Size([40, 10, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.layers.0.spatial_weighting.1.conv2.conv.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.layers.0.spatial_weighting.2.conv1.conv.weight - torch.Size([20, 80, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.layers.0.spatial_weighting.2.conv1.conv.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.layers.0.spatial_weighting.2.conv2.conv.weight - torch.Size([80, 20, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.layers.0.spatial_weighting.2.conv2.conv.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.layers.0.spatial_weighting.3.conv1.conv.weight - torch.Size([40, 160, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.layers.0.spatial_weighting.3.conv1.conv.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.layers.0.spatial_weighting.3.conv2.conv.weight - torch.Size([160, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.layers.0.spatial_weighting.3.conv2.conv.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.layers.1.cross_resolution_weighting.conv1.conv.weight - torch.Size([37, 300, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.layers.1.cross_resolution_weighting.conv1.bn.weight - torch.Size([37]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.layers.1.cross_resolution_weighting.conv1.bn.bias - torch.Size([37]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.layers.1.cross_resolution_weighting.conv2.conv.weight - torch.Size([300, 37, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.layers.1.cross_resolution_weighting.conv2.bn.weight - torch.Size([300]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.layers.1.cross_resolution_weighting.conv2.bn.bias - torch.Size([300]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.layers.1.depthwise_convs.0.conv.weight - torch.Size([20, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.layers.1.depthwise_convs.0.bn.weight - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.layers.1.depthwise_convs.0.bn.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.layers.1.depthwise_convs.1.conv.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.layers.1.depthwise_convs.1.bn.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.layers.1.depthwise_convs.1.bn.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.layers.1.depthwise_convs.2.conv.weight - torch.Size([80, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.layers.1.depthwise_convs.2.bn.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.layers.1.depthwise_convs.2.bn.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.layers.1.depthwise_convs.3.conv.weight - torch.Size([160, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.layers.1.depthwise_convs.3.bn.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.layers.1.depthwise_convs.3.bn.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.layers.1.spatial_weighting.0.conv1.conv.weight - torch.Size([5, 20, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.layers.1.spatial_weighting.0.conv1.conv.bias - torch.Size([5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.layers.1.spatial_weighting.0.conv2.conv.weight - torch.Size([20, 5, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.layers.1.spatial_weighting.0.conv2.conv.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.layers.1.spatial_weighting.1.conv1.conv.weight - torch.Size([10, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.layers.1.spatial_weighting.1.conv1.conv.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.layers.1.spatial_weighting.1.conv2.conv.weight - torch.Size([40, 10, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.layers.1.spatial_weighting.1.conv2.conv.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.layers.1.spatial_weighting.2.conv1.conv.weight - torch.Size([20, 80, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.layers.1.spatial_weighting.2.conv1.conv.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.layers.1.spatial_weighting.2.conv2.conv.weight - torch.Size([80, 20, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.layers.1.spatial_weighting.2.conv2.conv.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.layers.1.spatial_weighting.3.conv1.conv.weight - torch.Size([40, 160, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.layers.1.spatial_weighting.3.conv1.conv.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.layers.1.spatial_weighting.3.conv2.conv.weight - torch.Size([160, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.layers.1.spatial_weighting.3.conv2.conv.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.0.1.0.weight - torch.Size([40, 80, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.fuse_layers.0.1.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.0.1.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.0.2.0.weight - torch.Size([40, 160, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.fuse_layers.0.2.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.0.2.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.0.3.0.weight - torch.Size([40, 320, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.fuse_layers.0.3.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.0.3.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.1.0.0.0.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.fuse_layers.1.0.0.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.1.0.0.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.1.0.0.2.weight - torch.Size([80, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.fuse_layers.1.0.0.3.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.1.0.0.3.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.1.2.0.weight - torch.Size([80, 160, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.fuse_layers.1.2.1.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.1.2.1.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.1.3.0.weight - torch.Size([80, 320, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.fuse_layers.1.3.1.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.1.3.1.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.2.0.0.0.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.fuse_layers.2.0.0.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.2.0.0.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.2.0.0.2.weight - torch.Size([40, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.fuse_layers.2.0.0.3.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.2.0.0.3.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.2.0.1.0.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.fuse_layers.2.0.1.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.2.0.1.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.2.0.1.2.weight - torch.Size([160, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.fuse_layers.2.0.1.3.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.2.0.1.3.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.2.1.0.0.weight - torch.Size([80, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.fuse_layers.2.1.0.1.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.2.1.0.1.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.2.1.0.2.weight - torch.Size([160, 80, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.fuse_layers.2.1.0.3.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.2.1.0.3.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.2.3.0.weight - torch.Size([160, 320, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.fuse_layers.2.3.1.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.2.3.1.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.3.0.0.0.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.fuse_layers.3.0.0.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.3.0.0.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.3.0.0.2.weight - torch.Size([40, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.fuse_layers.3.0.0.3.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.3.0.0.3.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.3.0.1.0.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.fuse_layers.3.0.1.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.3.0.1.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.3.0.1.2.weight - torch.Size([40, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.fuse_layers.3.0.1.3.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.3.0.1.3.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.3.0.2.0.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.fuse_layers.3.0.2.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.3.0.2.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.3.0.2.2.weight - torch.Size([320, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.fuse_layers.3.0.2.3.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.3.0.2.3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.3.1.0.0.weight - torch.Size([80, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.fuse_layers.3.1.0.1.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.3.1.0.1.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.3.1.0.2.weight - torch.Size([80, 80, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.fuse_layers.3.1.0.3.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.3.1.0.3.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.3.1.1.0.weight - torch.Size([80, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.fuse_layers.3.1.1.1.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.3.1.1.1.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.3.1.1.2.weight - torch.Size([320, 80, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.fuse_layers.3.1.1.3.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.3.1.1.3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.3.2.0.0.weight - torch.Size([160, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.fuse_layers.3.2.0.1.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.3.2.0.1.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.3.2.0.2.weight - torch.Size([320, 160, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.0.fuse_layers.3.2.0.3.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.0.fuse_layers.3.2.0.3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.layers.0.cross_resolution_weighting.conv1.conv.weight - torch.Size([37, 300, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.layers.0.cross_resolution_weighting.conv1.bn.weight - torch.Size([37]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.layers.0.cross_resolution_weighting.conv1.bn.bias - torch.Size([37]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.layers.0.cross_resolution_weighting.conv2.conv.weight - torch.Size([300, 37, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.layers.0.cross_resolution_weighting.conv2.bn.weight - torch.Size([300]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.layers.0.cross_resolution_weighting.conv2.bn.bias - torch.Size([300]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.layers.0.depthwise_convs.0.conv.weight - torch.Size([20, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.layers.0.depthwise_convs.0.bn.weight - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.layers.0.depthwise_convs.0.bn.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.layers.0.depthwise_convs.1.conv.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.layers.0.depthwise_convs.1.bn.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.layers.0.depthwise_convs.1.bn.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.layers.0.depthwise_convs.2.conv.weight - torch.Size([80, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.layers.0.depthwise_convs.2.bn.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.layers.0.depthwise_convs.2.bn.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.layers.0.depthwise_convs.3.conv.weight - torch.Size([160, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.layers.0.depthwise_convs.3.bn.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.layers.0.depthwise_convs.3.bn.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.layers.0.spatial_weighting.0.conv1.conv.weight - torch.Size([5, 20, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.layers.0.spatial_weighting.0.conv1.conv.bias - torch.Size([5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.layers.0.spatial_weighting.0.conv2.conv.weight - torch.Size([20, 5, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.layers.0.spatial_weighting.0.conv2.conv.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.layers.0.spatial_weighting.1.conv1.conv.weight - torch.Size([10, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.layers.0.spatial_weighting.1.conv1.conv.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.layers.0.spatial_weighting.1.conv2.conv.weight - torch.Size([40, 10, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.layers.0.spatial_weighting.1.conv2.conv.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.layers.0.spatial_weighting.2.conv1.conv.weight - torch.Size([20, 80, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.layers.0.spatial_weighting.2.conv1.conv.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.layers.0.spatial_weighting.2.conv2.conv.weight - torch.Size([80, 20, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.layers.0.spatial_weighting.2.conv2.conv.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.layers.0.spatial_weighting.3.conv1.conv.weight - torch.Size([40, 160, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.layers.0.spatial_weighting.3.conv1.conv.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.layers.0.spatial_weighting.3.conv2.conv.weight - torch.Size([160, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.layers.0.spatial_weighting.3.conv2.conv.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.layers.1.cross_resolution_weighting.conv1.conv.weight - torch.Size([37, 300, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.layers.1.cross_resolution_weighting.conv1.bn.weight - torch.Size([37]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.layers.1.cross_resolution_weighting.conv1.bn.bias - torch.Size([37]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.layers.1.cross_resolution_weighting.conv2.conv.weight - torch.Size([300, 37, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.layers.1.cross_resolution_weighting.conv2.bn.weight - torch.Size([300]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.layers.1.cross_resolution_weighting.conv2.bn.bias - torch.Size([300]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.layers.1.depthwise_convs.0.conv.weight - torch.Size([20, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.layers.1.depthwise_convs.0.bn.weight - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.layers.1.depthwise_convs.0.bn.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.layers.1.depthwise_convs.1.conv.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.layers.1.depthwise_convs.1.bn.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.layers.1.depthwise_convs.1.bn.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.layers.1.depthwise_convs.2.conv.weight - torch.Size([80, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.layers.1.depthwise_convs.2.bn.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.layers.1.depthwise_convs.2.bn.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.layers.1.depthwise_convs.3.conv.weight - torch.Size([160, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.layers.1.depthwise_convs.3.bn.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.layers.1.depthwise_convs.3.bn.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.layers.1.spatial_weighting.0.conv1.conv.weight - torch.Size([5, 20, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.layers.1.spatial_weighting.0.conv1.conv.bias - torch.Size([5]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.layers.1.spatial_weighting.0.conv2.conv.weight - torch.Size([20, 5, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.layers.1.spatial_weighting.0.conv2.conv.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.layers.1.spatial_weighting.1.conv1.conv.weight - torch.Size([10, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.layers.1.spatial_weighting.1.conv1.conv.bias - torch.Size([10]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.layers.1.spatial_weighting.1.conv2.conv.weight - torch.Size([40, 10, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.layers.1.spatial_weighting.1.conv2.conv.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.layers.1.spatial_weighting.2.conv1.conv.weight - torch.Size([20, 80, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.layers.1.spatial_weighting.2.conv1.conv.bias - torch.Size([20]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.layers.1.spatial_weighting.2.conv2.conv.weight - torch.Size([80, 20, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.layers.1.spatial_weighting.2.conv2.conv.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.layers.1.spatial_weighting.3.conv1.conv.weight - torch.Size([40, 160, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.layers.1.spatial_weighting.3.conv1.conv.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.layers.1.spatial_weighting.3.conv2.conv.weight - torch.Size([160, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.layers.1.spatial_weighting.3.conv2.conv.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.0.1.0.weight - torch.Size([40, 80, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.fuse_layers.0.1.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.0.1.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.0.2.0.weight - torch.Size([40, 160, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.fuse_layers.0.2.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.0.2.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.0.3.0.weight - torch.Size([40, 320, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.fuse_layers.0.3.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.0.3.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.1.0.0.0.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.fuse_layers.1.0.0.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.1.0.0.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.1.0.0.2.weight - torch.Size([80, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.fuse_layers.1.0.0.3.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.1.0.0.3.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.1.2.0.weight - torch.Size([80, 160, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.fuse_layers.1.2.1.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.1.2.1.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.1.3.0.weight - torch.Size([80, 320, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.fuse_layers.1.3.1.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.1.3.1.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.2.0.0.0.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.fuse_layers.2.0.0.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.2.0.0.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.2.0.0.2.weight - torch.Size([40, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.fuse_layers.2.0.0.3.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.2.0.0.3.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.2.0.1.0.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.fuse_layers.2.0.1.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.2.0.1.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.2.0.1.2.weight - torch.Size([160, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.fuse_layers.2.0.1.3.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.2.0.1.3.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.2.1.0.0.weight - torch.Size([80, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.fuse_layers.2.1.0.1.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.2.1.0.1.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.2.1.0.2.weight - torch.Size([160, 80, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.fuse_layers.2.1.0.3.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.2.1.0.3.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.2.3.0.weight - torch.Size([160, 320, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.fuse_layers.2.3.1.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.2.3.1.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.3.0.0.0.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.fuse_layers.3.0.0.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.3.0.0.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.3.0.0.2.weight - torch.Size([40, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.fuse_layers.3.0.0.3.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.3.0.0.3.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.3.0.1.0.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.fuse_layers.3.0.1.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.3.0.1.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.3.0.1.2.weight - torch.Size([40, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.fuse_layers.3.0.1.3.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.3.0.1.3.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.3.0.2.0.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.fuse_layers.3.0.2.1.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.3.0.2.1.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.3.0.2.2.weight - torch.Size([320, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.fuse_layers.3.0.2.3.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.3.0.2.3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.3.1.0.0.weight - torch.Size([80, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.fuse_layers.3.1.0.1.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.3.1.0.1.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.3.1.0.2.weight - torch.Size([80, 80, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.fuse_layers.3.1.0.3.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.3.1.0.3.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.3.1.1.0.weight - torch.Size([80, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.fuse_layers.3.1.1.1.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.3.1.1.1.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.3.1.1.2.weight - torch.Size([320, 80, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.fuse_layers.3.1.1.3.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.3.1.1.3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.3.2.0.0.weight - torch.Size([160, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.fuse_layers.3.2.0.1.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.3.2.0.1.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.3.2.0.2.weight - torch.Size([320, 160, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.stage2.1.fuse_layers.3.2.0.3.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.stage2.1.fuse_layers.3.2.0.3.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.head_layer.projects.0.depthwise_conv.conv.weight - torch.Size([320, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.head_layer.projects.0.depthwise_conv.bn.weight - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.head_layer.projects.0.depthwise_conv.bn.bias - torch.Size([320]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.head_layer.projects.0.pointwise_conv.conv.weight - torch.Size([160, 320, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.head_layer.projects.0.pointwise_conv.bn.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.head_layer.projects.0.pointwise_conv.bn.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.head_layer.projects.1.depthwise_conv.conv.weight - torch.Size([160, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.head_layer.projects.1.depthwise_conv.bn.weight - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.head_layer.projects.1.depthwise_conv.bn.bias - torch.Size([160]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.head_layer.projects.1.pointwise_conv.conv.weight - torch.Size([80, 160, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.head_layer.projects.1.pointwise_conv.bn.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.head_layer.projects.1.pointwise_conv.bn.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.head_layer.projects.2.depthwise_conv.conv.weight - torch.Size([80, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.head_layer.projects.2.depthwise_conv.bn.weight - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.head_layer.projects.2.depthwise_conv.bn.bias - torch.Size([80]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.head_layer.projects.2.pointwise_conv.conv.weight - torch.Size([40, 80, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.head_layer.projects.2.pointwise_conv.bn.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.head_layer.projects.2.pointwise_conv.bn.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.head_layer.projects.3.depthwise_conv.conv.weight - torch.Size([40, 1, 3, 3]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.head_layer.projects.3.depthwise_conv.bn.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.head_layer.projects.3.depthwise_conv.bn.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.head_layer.projects.3.pointwise_conv.conv.weight - torch.Size([40, 40, 1, 1]): 
NormalInit: mean=0, std=0.001, bias=0 

backbone.head_layer.projects.3.pointwise_conv.bn.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.head_layer.projects.3.pointwise_conv.bn.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.conv_seg.weight - torch.Size([150, 40, 1, 1]): 
NormalInit: mean=0, std=0.01, bias=0 

decode_head.conv_seg.bias - torch.Size([150]): 
NormalInit: mean=0, std=0.01, bias=0 

decode_head.convs.0.conv.weight - torch.Size([40, 40, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.convs.0.bn.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.convs.0.bn.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.convs.1.conv.weight - torch.Size([40, 40, 3, 3]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.convs.1.bn.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.convs.1.bn.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.conv_cat.conv.weight - torch.Size([40, 80, 3, 3]): 
Initialized by user-defined `init_weights` in ConvModule  

decode_head.conv_cat.bn.weight - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.conv_cat.bn.bias - torch.Size([40]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  
2021-08-13 11:11:13,769 - mmseg - INFO - EncoderDecoder(
  (backbone): LiteHRNet(
    (stem): Stem(
      (conv1): ConvModule(
        (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn): SyncBatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
      (branch1): Sequential(
        (0): ConvModule(
          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)
          (bn): SyncBatchNorm(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (1): ConvModule(
          (conv): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn): SyncBatchNorm(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activate): ReLU(inplace=True)
        )
      )
      (expand_conv): ConvModule(
        (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): SyncBatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
      (depthwise_conv): ConvModule(
        (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)
        (bn): SyncBatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (linear_conv): ConvModule(
        (conv): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): SyncBatchNorm(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
    )
    (transition0): ModuleList(
      (0): Sequential(
        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (1): SyncBatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): Conv2d(32, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (4): ReLU()
      )
      (1): Sequential(
        (0): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)
          (1): SyncBatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Conv2d(32, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU()
        )
      )
    )
    (stage0): Sequential(
      (0): LiteHRModule(
        (layers): Sequential(
          (0): ConditionalChannelWeighting(
            (cross_resolution_weighting): CrossResolutionWeighting(
              (conv1): ConvModule(
                (conv): Conv2d(60, 7, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn): SyncBatchNorm(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (activate): ReLU(inplace=True)
              )
              (conv2): ConvModule(
                (conv): Conv2d(7, 60, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn): SyncBatchNorm(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (activate): Sigmoid()
              )
            )
            (depthwise_convs): ModuleList(
              (0): ConvModule(
                (conv): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=20, bias=False)
                (bn): SyncBatchNorm(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (1): ConvModule(
                (conv): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)
                (bn): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (spatial_weighting): ModuleList(
              (0): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(20, 5, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(5, 20, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
              (1): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(40, 10, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(10, 40, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
            )
          )
          (1): ConditionalChannelWeighting(
            (cross_resolution_weighting): CrossResolutionWeighting(
              (conv1): ConvModule(
                (conv): Conv2d(60, 7, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn): SyncBatchNorm(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (activate): ReLU(inplace=True)
              )
              (conv2): ConvModule(
                (conv): Conv2d(7, 60, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn): SyncBatchNorm(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (activate): Sigmoid()
              )
            )
            (depthwise_convs): ModuleList(
              (0): ConvModule(
                (conv): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=20, bias=False)
                (bn): SyncBatchNorm(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (1): ConvModule(
                (conv): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)
                (bn): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (spatial_weighting): ModuleList(
              (0): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(20, 5, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(5, 20, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
              (1): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(40, 10, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(10, 40, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
            )
          )
        )
        (fuse_layers): ModuleList(
          (0): ModuleList(
            (0): None
            (1): Sequential(
              (0): Conv2d(80, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=nearest)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=40, bias=False)
                (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Conv2d(40, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (3): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): None
          )
        )
        (relu): ReLU()
      )
      (1): LiteHRModule(
        (layers): Sequential(
          (0): ConditionalChannelWeighting(
            (cross_resolution_weighting): CrossResolutionWeighting(
              (conv1): ConvModule(
                (conv): Conv2d(60, 7, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn): SyncBatchNorm(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (activate): ReLU(inplace=True)
              )
              (conv2): ConvModule(
                (conv): Conv2d(7, 60, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn): SyncBatchNorm(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (activate): Sigmoid()
              )
            )
            (depthwise_convs): ModuleList(
              (0): ConvModule(
                (conv): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=20, bias=False)
                (bn): SyncBatchNorm(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (1): ConvModule(
                (conv): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)
                (bn): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (spatial_weighting): ModuleList(
              (0): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(20, 5, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(5, 20, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
              (1): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(40, 10, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(10, 40, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
            )
          )
          (1): ConditionalChannelWeighting(
            (cross_resolution_weighting): CrossResolutionWeighting(
              (conv1): ConvModule(
                (conv): Conv2d(60, 7, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn): SyncBatchNorm(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (activate): ReLU(inplace=True)
              )
              (conv2): ConvModule(
                (conv): Conv2d(7, 60, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn): SyncBatchNorm(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (activate): Sigmoid()
              )
            )
            (depthwise_convs): ModuleList(
              (0): ConvModule(
                (conv): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=20, bias=False)
                (bn): SyncBatchNorm(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (1): ConvModule(
                (conv): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)
                (bn): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (spatial_weighting): ModuleList(
              (0): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(20, 5, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(5, 20, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
              (1): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(40, 10, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(10, 40, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
            )
          )
        )
        (fuse_layers): ModuleList(
          (0): ModuleList(
            (0): None
            (1): Sequential(
              (0): Conv2d(80, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=nearest)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=40, bias=False)
                (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Conv2d(40, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (3): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): None
          )
        )
        (relu): ReLU()
      )
    )
    (transition1): ModuleList(
      (0): None
      (1): None
      (2): Sequential(
        (0): Sequential(
          (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=80, bias=False)
          (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU()
        )
      )
    )
    (stage1): Sequential(
      (0): LiteHRModule(
        (layers): Sequential(
          (0): ConditionalChannelWeighting(
            (cross_resolution_weighting): CrossResolutionWeighting(
              (conv1): ConvModule(
                (conv): Conv2d(140, 17, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn): SyncBatchNorm(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (activate): ReLU(inplace=True)
              )
              (conv2): ConvModule(
                (conv): Conv2d(17, 140, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn): SyncBatchNorm(140, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (activate): Sigmoid()
              )
            )
            (depthwise_convs): ModuleList(
              (0): ConvModule(
                (conv): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=20, bias=False)
                (bn): SyncBatchNorm(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (1): ConvModule(
                (conv): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)
                (bn): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (2): ConvModule(
                (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)
                (bn): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (spatial_weighting): ModuleList(
              (0): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(20, 5, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(5, 20, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
              (1): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(40, 10, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(10, 40, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
              (2): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(80, 20, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(20, 80, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
            )
          )
          (1): ConditionalChannelWeighting(
            (cross_resolution_weighting): CrossResolutionWeighting(
              (conv1): ConvModule(
                (conv): Conv2d(140, 17, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn): SyncBatchNorm(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (activate): ReLU(inplace=True)
              )
              (conv2): ConvModule(
                (conv): Conv2d(17, 140, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn): SyncBatchNorm(140, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (activate): Sigmoid()
              )
            )
            (depthwise_convs): ModuleList(
              (0): ConvModule(
                (conv): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=20, bias=False)
                (bn): SyncBatchNorm(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (1): ConvModule(
                (conv): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)
                (bn): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (2): ConvModule(
                (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)
                (bn): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (spatial_weighting): ModuleList(
              (0): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(20, 5, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(5, 20, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
              (1): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(40, 10, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(10, 40, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
              (2): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(80, 20, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(20, 80, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
            )
          )
        )
        (fuse_layers): ModuleList(
          (0): ModuleList(
            (0): None
            (1): Sequential(
              (0): Conv2d(80, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=nearest)
            )
            (2): Sequential(
              (0): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=nearest)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=40, bias=False)
                (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Conv2d(40, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (3): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): None
            (2): Sequential(
              (0): Conv2d(160, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=nearest)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=40, bias=False)
                (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (3): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (4): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=40, bias=False)
                (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (3): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=80, bias=False)
                (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (3): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (2): None
          )
        )
        (relu): ReLU()
      )
      (1): LiteHRModule(
        (layers): Sequential(
          (0): ConditionalChannelWeighting(
            (cross_resolution_weighting): CrossResolutionWeighting(
              (conv1): ConvModule(
                (conv): Conv2d(140, 17, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn): SyncBatchNorm(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (activate): ReLU(inplace=True)
              )
              (conv2): ConvModule(
                (conv): Conv2d(17, 140, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn): SyncBatchNorm(140, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (activate): Sigmoid()
              )
            )
            (depthwise_convs): ModuleList(
              (0): ConvModule(
                (conv): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=20, bias=False)
                (bn): SyncBatchNorm(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (1): ConvModule(
                (conv): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)
                (bn): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (2): ConvModule(
                (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)
                (bn): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (spatial_weighting): ModuleList(
              (0): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(20, 5, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(5, 20, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
              (1): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(40, 10, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(10, 40, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
              (2): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(80, 20, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(20, 80, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
            )
          )
          (1): ConditionalChannelWeighting(
            (cross_resolution_weighting): CrossResolutionWeighting(
              (conv1): ConvModule(
                (conv): Conv2d(140, 17, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn): SyncBatchNorm(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (activate): ReLU(inplace=True)
              )
              (conv2): ConvModule(
                (conv): Conv2d(17, 140, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn): SyncBatchNorm(140, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (activate): Sigmoid()
              )
            )
            (depthwise_convs): ModuleList(
              (0): ConvModule(
                (conv): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=20, bias=False)
                (bn): SyncBatchNorm(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (1): ConvModule(
                (conv): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)
                (bn): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (2): ConvModule(
                (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)
                (bn): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (spatial_weighting): ModuleList(
              (0): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(20, 5, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(5, 20, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
              (1): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(40, 10, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(10, 40, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
              (2): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(80, 20, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(20, 80, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
            )
          )
        )
        (fuse_layers): ModuleList(
          (0): ModuleList(
            (0): None
            (1): Sequential(
              (0): Conv2d(80, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=nearest)
            )
            (2): Sequential(
              (0): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=nearest)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=40, bias=False)
                (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Conv2d(40, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (3): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): None
            (2): Sequential(
              (0): Conv2d(160, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=nearest)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=40, bias=False)
                (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (3): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (4): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=40, bias=False)
                (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (3): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=80, bias=False)
                (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (3): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (2): None
          )
        )
        (relu): ReLU()
      )
      (2): LiteHRModule(
        (layers): Sequential(
          (0): ConditionalChannelWeighting(
            (cross_resolution_weighting): CrossResolutionWeighting(
              (conv1): ConvModule(
                (conv): Conv2d(140, 17, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn): SyncBatchNorm(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (activate): ReLU(inplace=True)
              )
              (conv2): ConvModule(
                (conv): Conv2d(17, 140, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn): SyncBatchNorm(140, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (activate): Sigmoid()
              )
            )
            (depthwise_convs): ModuleList(
              (0): ConvModule(
                (conv): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=20, bias=False)
                (bn): SyncBatchNorm(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (1): ConvModule(
                (conv): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)
                (bn): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (2): ConvModule(
                (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)
                (bn): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (spatial_weighting): ModuleList(
              (0): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(20, 5, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(5, 20, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
              (1): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(40, 10, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(10, 40, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
              (2): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(80, 20, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(20, 80, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
            )
          )
          (1): ConditionalChannelWeighting(
            (cross_resolution_weighting): CrossResolutionWeighting(
              (conv1): ConvModule(
                (conv): Conv2d(140, 17, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn): SyncBatchNorm(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (activate): ReLU(inplace=True)
              )
              (conv2): ConvModule(
                (conv): Conv2d(17, 140, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn): SyncBatchNorm(140, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (activate): Sigmoid()
              )
            )
            (depthwise_convs): ModuleList(
              (0): ConvModule(
                (conv): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=20, bias=False)
                (bn): SyncBatchNorm(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (1): ConvModule(
                (conv): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)
                (bn): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (2): ConvModule(
                (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)
                (bn): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (spatial_weighting): ModuleList(
              (0): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(20, 5, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(5, 20, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
              (1): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(40, 10, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(10, 40, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
              (2): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(80, 20, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(20, 80, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
            )
          )
        )
        (fuse_layers): ModuleList(
          (0): ModuleList(
            (0): None
            (1): Sequential(
              (0): Conv2d(80, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=nearest)
            )
            (2): Sequential(
              (0): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=nearest)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=40, bias=False)
                (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Conv2d(40, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (3): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): None
            (2): Sequential(
              (0): Conv2d(160, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=nearest)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=40, bias=False)
                (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (3): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (4): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=40, bias=False)
                (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (3): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=80, bias=False)
                (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (3): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (2): None
          )
        )
        (relu): ReLU()
      )
      (3): LiteHRModule(
        (layers): Sequential(
          (0): ConditionalChannelWeighting(
            (cross_resolution_weighting): CrossResolutionWeighting(
              (conv1): ConvModule(
                (conv): Conv2d(140, 17, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn): SyncBatchNorm(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (activate): ReLU(inplace=True)
              )
              (conv2): ConvModule(
                (conv): Conv2d(17, 140, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn): SyncBatchNorm(140, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (activate): Sigmoid()
              )
            )
            (depthwise_convs): ModuleList(
              (0): ConvModule(
                (conv): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=20, bias=False)
                (bn): SyncBatchNorm(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (1): ConvModule(
                (conv): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)
                (bn): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (2): ConvModule(
                (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)
                (bn): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (spatial_weighting): ModuleList(
              (0): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(20, 5, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(5, 20, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
              (1): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(40, 10, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(10, 40, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
              (2): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(80, 20, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(20, 80, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
            )
          )
          (1): ConditionalChannelWeighting(
            (cross_resolution_weighting): CrossResolutionWeighting(
              (conv1): ConvModule(
                (conv): Conv2d(140, 17, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn): SyncBatchNorm(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (activate): ReLU(inplace=True)
              )
              (conv2): ConvModule(
                (conv): Conv2d(17, 140, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn): SyncBatchNorm(140, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (activate): Sigmoid()
              )
            )
            (depthwise_convs): ModuleList(
              (0): ConvModule(
                (conv): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=20, bias=False)
                (bn): SyncBatchNorm(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (1): ConvModule(
                (conv): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)
                (bn): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (2): ConvModule(
                (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)
                (bn): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (spatial_weighting): ModuleList(
              (0): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(20, 5, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(5, 20, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
              (1): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(40, 10, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(10, 40, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
              (2): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(80, 20, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(20, 80, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
            )
          )
        )
        (fuse_layers): ModuleList(
          (0): ModuleList(
            (0): None
            (1): Sequential(
              (0): Conv2d(80, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=nearest)
            )
            (2): Sequential(
              (0): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=nearest)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=40, bias=False)
                (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Conv2d(40, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (3): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): None
            (2): Sequential(
              (0): Conv2d(160, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=nearest)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=40, bias=False)
                (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (3): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (4): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=40, bias=False)
                (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (3): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=80, bias=False)
                (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (3): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (2): None
          )
        )
        (relu): ReLU()
      )
    )
    (transition2): ModuleList(
      (0): None
      (1): None
      (2): None
      (3): Sequential(
        (0): Sequential(
          (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=160, bias=False)
          (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): SyncBatchNorm(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU()
        )
      )
    )
    (stage2): Sequential(
      (0): LiteHRModule(
        (layers): Sequential(
          (0): ConditionalChannelWeighting(
            (cross_resolution_weighting): CrossResolutionWeighting(
              (conv1): ConvModule(
                (conv): Conv2d(300, 37, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn): SyncBatchNorm(37, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (activate): ReLU(inplace=True)
              )
              (conv2): ConvModule(
                (conv): Conv2d(37, 300, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn): SyncBatchNorm(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (activate): Sigmoid()
              )
            )
            (depthwise_convs): ModuleList(
              (0): ConvModule(
                (conv): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=20, bias=False)
                (bn): SyncBatchNorm(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (1): ConvModule(
                (conv): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)
                (bn): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (2): ConvModule(
                (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)
                (bn): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (3): ConvModule(
                (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)
                (bn): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (spatial_weighting): ModuleList(
              (0): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(20, 5, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(5, 20, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
              (1): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(40, 10, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(10, 40, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
              (2): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(80, 20, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(20, 80, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
              (3): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
            )
          )
          (1): ConditionalChannelWeighting(
            (cross_resolution_weighting): CrossResolutionWeighting(
              (conv1): ConvModule(
                (conv): Conv2d(300, 37, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn): SyncBatchNorm(37, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (activate): ReLU(inplace=True)
              )
              (conv2): ConvModule(
                (conv): Conv2d(37, 300, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn): SyncBatchNorm(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (activate): Sigmoid()
              )
            )
            (depthwise_convs): ModuleList(
              (0): ConvModule(
                (conv): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=20, bias=False)
                (bn): SyncBatchNorm(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (1): ConvModule(
                (conv): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)
                (bn): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (2): ConvModule(
                (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)
                (bn): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (3): ConvModule(
                (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)
                (bn): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (spatial_weighting): ModuleList(
              (0): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(20, 5, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(5, 20, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
              (1): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(40, 10, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(10, 40, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
              (2): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(80, 20, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(20, 80, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
              (3): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
            )
          )
        )
        (fuse_layers): ModuleList(
          (0): ModuleList(
            (0): None
            (1): Sequential(
              (0): Conv2d(80, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=nearest)
            )
            (2): Sequential(
              (0): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=nearest)
            )
            (3): Sequential(
              (0): Conv2d(320, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=nearest)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=40, bias=False)
                (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Conv2d(40, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (3): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): None
            (2): Sequential(
              (0): Conv2d(160, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=nearest)
            )
            (3): Sequential(
              (0): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=nearest)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=40, bias=False)
                (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (3): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (4): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=40, bias=False)
                (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (3): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=80, bias=False)
                (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (3): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (2): None
            (3): Sequential(
              (0): Conv2d(320, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=nearest)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=40, bias=False)
                (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (3): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (4): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=40, bias=False)
                (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (3): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (4): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=40, bias=False)
                (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Conv2d(40, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (3): SyncBatchNorm(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=80, bias=False)
                (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (3): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (4): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=80, bias=False)
                (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (3): SyncBatchNorm(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=160, bias=False)
                (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (3): SyncBatchNorm(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (3): None
          )
        )
        (relu): ReLU()
      )
      (1): LiteHRModule(
        (layers): Sequential(
          (0): ConditionalChannelWeighting(
            (cross_resolution_weighting): CrossResolutionWeighting(
              (conv1): ConvModule(
                (conv): Conv2d(300, 37, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn): SyncBatchNorm(37, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (activate): ReLU(inplace=True)
              )
              (conv2): ConvModule(
                (conv): Conv2d(37, 300, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn): SyncBatchNorm(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (activate): Sigmoid()
              )
            )
            (depthwise_convs): ModuleList(
              (0): ConvModule(
                (conv): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=20, bias=False)
                (bn): SyncBatchNorm(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (1): ConvModule(
                (conv): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)
                (bn): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (2): ConvModule(
                (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)
                (bn): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (3): ConvModule(
                (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)
                (bn): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (spatial_weighting): ModuleList(
              (0): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(20, 5, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(5, 20, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
              (1): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(40, 10, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(10, 40, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
              (2): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(80, 20, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(20, 80, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
              (3): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
            )
          )
          (1): ConditionalChannelWeighting(
            (cross_resolution_weighting): CrossResolutionWeighting(
              (conv1): ConvModule(
                (conv): Conv2d(300, 37, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn): SyncBatchNorm(37, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (activate): ReLU(inplace=True)
              )
              (conv2): ConvModule(
                (conv): Conv2d(37, 300, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (bn): SyncBatchNorm(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (activate): Sigmoid()
              )
            )
            (depthwise_convs): ModuleList(
              (0): ConvModule(
                (conv): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=20, bias=False)
                (bn): SyncBatchNorm(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (1): ConvModule(
                (conv): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)
                (bn): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (2): ConvModule(
                (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)
                (bn): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
              (3): ConvModule(
                (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)
                (bn): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (spatial_weighting): ModuleList(
              (0): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(20, 5, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(5, 20, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
              (1): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(40, 10, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(10, 40, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
              (2): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(80, 20, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(20, 80, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
              (3): SpatialWeighting(
                (global_avgpool): AdaptiveAvgPool2d(output_size=1)
                (conv1): ConvModule(
                  (conv): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1))
                  (activate): ReLU(inplace=True)
                )
                (conv2): ConvModule(
                  (conv): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1))
                  (activate): Sigmoid()
                )
              )
            )
          )
        )
        (fuse_layers): ModuleList(
          (0): ModuleList(
            (0): None
            (1): Sequential(
              (0): Conv2d(80, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=nearest)
            )
            (2): Sequential(
              (0): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=nearest)
            )
            (3): Sequential(
              (0): Conv2d(320, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=8.0, mode=nearest)
            )
          )
          (1): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=40, bias=False)
                (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Conv2d(40, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (3): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): None
            (2): Sequential(
              (0): Conv2d(160, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=nearest)
            )
            (3): Sequential(
              (0): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=4.0, mode=nearest)
            )
          )
          (2): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=40, bias=False)
                (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (3): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (4): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=40, bias=False)
                (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (3): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=80, bias=False)
                (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Conv2d(80, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (3): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (2): None
            (3): Sequential(
              (0): Conv2d(320, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): Upsample(scale_factor=2.0, mode=nearest)
            )
          )
          (3): ModuleList(
            (0): Sequential(
              (0): Sequential(
                (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=40, bias=False)
                (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (3): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (4): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=40, bias=False)
                (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (3): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (4): ReLU(inplace=True)
              )
              (2): Sequential(
                (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=40, bias=False)
                (1): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Conv2d(40, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (3): SyncBatchNorm(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (1): Sequential(
              (0): Sequential(
                (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=80, bias=False)
                (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (3): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (4): ReLU(inplace=True)
              )
              (1): Sequential(
                (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=80, bias=False)
                (1): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (3): SyncBatchNorm(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (2): Sequential(
              (0): Sequential(
                (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=160, bias=False)
                (1): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (3): SyncBatchNorm(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              )
            )
            (3): None
          )
        )
        (relu): ReLU()
      )
    )
    (head_layer): IterativeHead(
      (projects): ModuleList(
        (0): DepthwiseSeparableConvModule(
          (depthwise_conv): ConvModule(
            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)
            (bn): SyncBatchNorm(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (pointwise_conv): ConvModule(
            (conv): Conv2d(320, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(inplace=True)
          )
        )
        (1): DepthwiseSeparableConvModule(
          (depthwise_conv): ConvModule(
            (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)
            (bn): SyncBatchNorm(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (pointwise_conv): ConvModule(
            (conv): Conv2d(160, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(inplace=True)
          )
        )
        (2): DepthwiseSeparableConvModule(
          (depthwise_conv): ConvModule(
            (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)
            (bn): SyncBatchNorm(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (pointwise_conv): ConvModule(
            (conv): Conv2d(80, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(inplace=True)
          )
        )
        (3): DepthwiseSeparableConvModule(
          (depthwise_conv): ConvModule(
            (conv): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)
            (bn): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (pointwise_conv): ConvModule(
            (conv): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (bn): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (activate): ReLU(inplace=True)
          )
        )
      )
    )
  )
  init_cfg=[{'type': 'Normal', 'std': 0.001, 'layer': 'Conv2d'}, {'type': 'Constant', 'val': 1, 'layer': ['_BatchNorm', 'GroupNorm']}]
  (decode_head): FCNHead(
    input_transform=None, ignore_index=255, align_corners=False
    (loss_decode): CrossEntropyLoss()
    (conv_seg): Conv2d(40, 150, kernel_size=(1, 1), stride=(1, 1))
    (dropout): Dropout2d(p=0.1, inplace=False)
    (convs): Sequential(
      (0): ConvModule(
        (conv): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
      (1): ConvModule(
        (conv): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (activate): ReLU(inplace=True)
      )
    )
    (conv_cat): ConvModule(
      (conv): Conv2d(80, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): SyncBatchNorm(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (activate): ReLU(inplace=True)
    )
  )
  init_cfg={'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
)
2021-08-13 11:11:14,416 - mmseg - INFO - Loaded 20210 images
2021-08-13 11:11:19,646 - mmseg - INFO - Loaded 2000 images
2021-08-13 11:11:19,647 - mmseg - INFO - Start running, host: hejunjun@SH-IDC2-172-20-20-69, work_dir: /mnt/lustrenew/hejunjun/mmseg_dev/lite_hrnet/mmsegmentation/work_dirs/fcn_litehr18-with-head_512x512_160k_ade20k
2021-08-13 11:11:19,647 - mmseg - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_iter:
(VERY_HIGH   ) PolyLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
2021-08-13 11:11:19,648 - mmseg - INFO - workflow: [('train', 1)], max: 160000 iters
2021-08-13 11:12:39,029 - mmseg - INFO - Iter [50/160000]	lr: 9.997e-03, eta: 1 day, 18:51:08, time: 0.964, data_time: 0.012, memory: 4496, decode.loss_seg: 3.4824, decode.acc_seg: 15.2488, loss: 3.4824
2021-08-13 11:13:21,410 - mmseg - INFO - Iter [100/160000]	lr: 9.994e-03, eta: 1 day, 16:14:29, time: 0.848, data_time: 0.010, memory: 4496, decode.loss_seg: 2.9230, decode.acc_seg: 19.4131, loss: 2.9230
2021-08-13 11:14:04,255 - mmseg - INFO - Iter [150/160000]	lr: 9.992e-03, eta: 1 day, 15:30:09, time: 0.857, data_time: 0.010, memory: 4496, decode.loss_seg: 2.8082, decode.acc_seg: 20.1573, loss: 2.8082
2021-08-13 11:14:48,287 - mmseg - INFO - Iter [200/160000]	lr: 9.989e-03, eta: 1 day, 15:23:15, time: 0.880, data_time: 0.010, memory: 4496, decode.loss_seg: 2.6628, decode.acc_seg: 23.1390, loss: 2.6628
2021-08-13 11:15:32,349 - mmseg - INFO - Iter [250/160000]	lr: 9.986e-03, eta: 1 day, 15:19:19, time: 0.881, data_time: 0.010, memory: 4496, decode.loss_seg: 2.5453, decode.acc_seg: 24.9495, loss: 2.5453
2021-08-13 11:16:15,709 - mmseg - INFO - Iter [300/160000]	lr: 9.983e-03, eta: 1 day, 15:10:11, time: 0.867, data_time: 0.010, memory: 4496, decode.loss_seg: 2.4762, decode.acc_seg: 25.6802, loss: 2.4762
2021-08-13 11:16:59,274 - mmseg - INFO - Iter [350/160000]	lr: 9.981e-03, eta: 1 day, 15:04:54, time: 0.871, data_time: 0.010, memory: 4496, decode.loss_seg: 2.4603, decode.acc_seg: 25.4192, loss: 2.4603
2021-08-13 11:17:41,672 - mmseg - INFO - Iter [400/160000]	lr: 9.978e-03, eta: 1 day, 14:53:14, time: 0.848, data_time: 0.010, memory: 4496, decode.loss_seg: 2.4186, decode.acc_seg: 27.2760, loss: 2.4186
2021-08-13 11:18:24,667 - mmseg - INFO - Iter [450/160000]	lr: 9.975e-03, eta: 1 day, 14:47:20, time: 0.860, data_time: 0.009, memory: 4496, decode.loss_seg: 2.3481, decode.acc_seg: 26.9961, loss: 2.3481
2021-08-13 11:19:07,817 - mmseg - INFO - Iter [500/160000]	lr: 9.972e-03, eta: 1 day, 14:43:22, time: 0.863, data_time: 0.009, memory: 4496, decode.loss_seg: 2.3462, decode.acc_seg: 28.3364, loss: 2.3462
2021-08-13 11:19:51,851 - mmseg - INFO - Iter [550/160000]	lr: 9.969e-03, eta: 1 day, 14:44:11, time: 0.880, data_time: 0.010, memory: 4496, decode.loss_seg: 2.3039, decode.acc_seg: 28.6267, loss: 2.3039
2021-08-13 11:20:35,713 - mmseg - INFO - Iter [600/160000]	lr: 9.967e-03, eta: 1 day, 14:44:05, time: 0.877, data_time: 0.010, memory: 4496, decode.loss_seg: 2.3194, decode.acc_seg: 29.8633, loss: 2.3194
2021-08-13 11:21:53,913 - mmseg - INFO - Iter [650/160000]	lr: 9.964e-03, eta: 1 day, 17:04:05, time: 1.564, data_time: 0.685, memory: 4496, decode.loss_seg: 2.2906, decode.acc_seg: 29.1931, loss: 2.2906
2021-08-13 11:22:38,375 - mmseg - INFO - Iter [700/160000]	lr: 9.961e-03, eta: 1 day, 16:56:02, time: 0.889, data_time: 0.010, memory: 4496, decode.loss_seg: 2.2571, decode.acc_seg: 29.4430, loss: 2.2571
2021-08-13 11:23:21,765 - mmseg - INFO - Iter [750/160000]	lr: 9.958e-03, eta: 1 day, 16:45:13, time: 0.868, data_time: 0.010, memory: 4496, decode.loss_seg: 2.2659, decode.acc_seg: 30.2411, loss: 2.2659
2021-08-13 11:24:04,307 - mmseg - INFO - Iter [800/160000]	lr: 9.955e-03, eta: 1 day, 16:32:43, time: 0.851, data_time: 0.009, memory: 4496, decode.loss_seg: 2.1655, decode.acc_seg: 32.6245, loss: 2.1655
2021-08-13 11:24:47,130 - mmseg - INFO - Iter [850/160000]	lr: 9.953e-03, eta: 1 day, 16:22:34, time: 0.857, data_time: 0.010, memory: 4496, decode.loss_seg: 2.1484, decode.acc_seg: 31.2002, loss: 2.1484
2021-08-13 11:25:32,001 - mmseg - INFO - Iter [900/160000]	lr: 9.950e-03, eta: 1 day, 16:19:24, time: 0.897, data_time: 0.009, memory: 4496, decode.loss_seg: 2.1600, decode.acc_seg: 31.0796, loss: 2.1600
2021-08-13 11:26:14,875 - mmseg - INFO - Iter [950/160000]	lr: 9.947e-03, eta: 1 day, 16:10:59, time: 0.858, data_time: 0.010, memory: 4496, decode.loss_seg: 2.1087, decode.acc_seg: 31.6571, loss: 2.1087
2021-08-13 11:26:58,783 - mmseg - INFO - Exp name: fcn_litehr18-with-head_512x512_160k_ade20k.py
2021-08-13 11:26:58,783 - mmseg - INFO - Iter [1000/160000]	lr: 9.944e-03, eta: 1 day, 16:06:07, time: 0.878, data_time: 0.010, memory: 4496, decode.loss_seg: 2.1491, decode.acc_seg: 32.4982, loss: 2.1491
2021-08-13 11:27:41,321 - mmseg - INFO - Iter [1050/160000]	lr: 9.942e-03, eta: 1 day, 15:58:08, time: 0.851, data_time: 0.009, memory: 4496, decode.loss_seg: 2.1245, decode.acc_seg: 31.1714, loss: 2.1245
2021-08-13 11:28:24,656 - mmseg - INFO - Iter [1100/160000]	lr: 9.939e-03, eta: 1 day, 15:52:45, time: 0.867, data_time: 0.010, memory: 4496, decode.loss_seg: 2.0968, decode.acc_seg: 32.5642, loss: 2.0968
2021-08-13 11:29:07,003 - mmseg - INFO - Iter [1150/160000]	lr: 9.936e-03, eta: 1 day, 15:45:29, time: 0.847, data_time: 0.010, memory: 4496, decode.loss_seg: 2.0947, decode.acc_seg: 32.8396, loss: 2.0947
2021-08-13 11:29:49,241 - mmseg - INFO - Iter [1200/160000]	lr: 9.933e-03, eta: 1 day, 15:38:33, time: 0.845, data_time: 0.010, memory: 4496, decode.loss_seg: 2.0660, decode.acc_seg: 33.2725, loss: 2.0660
2021-08-13 11:30:31,402 - mmseg - INFO - Iter [1250/160000]	lr: 9.930e-03, eta: 1 day, 15:31:54, time: 0.843, data_time: 0.009, memory: 4496, decode.loss_seg: 2.0173, decode.acc_seg: 34.4655, loss: 2.0173
2021-08-13 11:31:47,807 - mmseg - INFO - Iter [1300/160000]	lr: 9.928e-03, eta: 1 day, 16:35:23, time: 1.528, data_time: 0.700, memory: 4496, decode.loss_seg: 2.0812, decode.acc_seg: 32.9656, loss: 2.0812
2021-08-13 11:32:31,114 - mmseg - INFO - Iter [1350/160000]	lr: 9.925e-03, eta: 1 day, 16:29:19, time: 0.867, data_time: 0.010, memory: 4496, decode.loss_seg: 2.0143, decode.acc_seg: 34.4606, loss: 2.0143
2021-08-13 11:33:14,295 - mmseg - INFO - Iter [1400/160000]	lr: 9.922e-03, eta: 1 day, 16:23:20, time: 0.863, data_time: 0.009, memory: 4496, decode.loss_seg: 2.0479, decode.acc_seg: 33.2722, loss: 2.0479
2021-08-13 11:33:56,654 - mmseg - INFO - Iter [1450/160000]	lr: 9.919e-03, eta: 1 day, 16:16:13, time: 0.847, data_time: 0.010, memory: 4496, decode.loss_seg: 1.9750, decode.acc_seg: 34.1461, loss: 1.9750
2021-08-13 11:34:40,626 - mmseg - INFO - Iter [1500/160000]	lr: 9.916e-03, eta: 1 day, 16:12:20, time: 0.879, data_time: 0.010, memory: 4496, decode.loss_seg: 2.0105, decode.acc_seg: 33.7192, loss: 2.0105
2021-08-13 11:35:25,891 - mmseg - INFO - Iter [1550/160000]	lr: 9.914e-03, eta: 1 day, 16:10:54, time: 0.905, data_time: 0.010, memory: 4496, decode.loss_seg: 1.9291, decode.acc_seg: 35.4218, loss: 1.9291
2021-08-13 11:36:10,313 - mmseg - INFO - Iter [1600/160000]	lr: 9.911e-03, eta: 1 day, 16:08:08, time: 0.889, data_time: 0.010, memory: 4496, decode.loss_seg: 1.9936, decode.acc_seg: 34.0334, loss: 1.9936
2021-08-13 11:36:54,477 - mmseg - INFO - Iter [1650/160000]	lr: 9.908e-03, eta: 1 day, 16:05:03, time: 0.883, data_time: 0.010, memory: 4496, decode.loss_seg: 1.9190, decode.acc_seg: 35.5063, loss: 1.9190
2021-08-13 11:37:37,529 - mmseg - INFO - Iter [1700/160000]	lr: 9.905e-03, eta: 1 day, 16:00:26, time: 0.862, data_time: 0.010, memory: 4496, decode.loss_seg: 1.9571, decode.acc_seg: 35.5205, loss: 1.9571
2021-08-13 11:38:20,000 - mmseg - INFO - Iter [1750/160000]	lr: 9.903e-03, eta: 1 day, 15:55:08, time: 0.849, data_time: 0.010, memory: 4496, decode.loss_seg: 1.9503, decode.acc_seg: 35.7499, loss: 1.9503
2021-08-13 11:39:03,038 - mmseg - INFO - Iter [1800/160000]	lr: 9.900e-03, eta: 1 day, 15:50:53, time: 0.861, data_time: 0.010, memory: 4496, decode.loss_seg: 1.9339, decode.acc_seg: 35.9038, loss: 1.9339
2021-08-13 11:39:45,602 - mmseg - INFO - Iter [1850/160000]	lr: 9.897e-03, eta: 1 day, 15:46:12, time: 0.852, data_time: 0.010, memory: 4496, decode.loss_seg: 1.9234, decode.acc_seg: 35.6288, loss: 1.9234
2021-08-13 11:41:03,231 - mmseg - INFO - Iter [1900/160000]	lr: 9.894e-03, eta: 1 day, 16:30:19, time: 1.552, data_time: 0.726, memory: 4496, decode.loss_seg: 1.9490, decode.acc_seg: 36.0848, loss: 1.9490
2021-08-13 11:41:48,335 - mmseg - INFO - Iter [1950/160000]	lr: 9.891e-03, eta: 1 day, 16:28:08, time: 0.901, data_time: 0.010, memory: 4496, decode.loss_seg: 1.8911, decode.acc_seg: 34.8767, loss: 1.8911
2021-08-13 11:42:31,518 - mmseg - INFO - Exp name: fcn_litehr18-with-head_512x512_160k_ade20k.py
2021-08-13 11:42:31,518 - mmseg - INFO - Iter [2000/160000]	lr: 9.889e-03, eta: 1 day, 16:23:35, time: 0.864, data_time: 0.010, memory: 4496, decode.loss_seg: 1.9566, decode.acc_seg: 35.4333, loss: 1.9566
2021-08-13 11:43:13,793 - mmseg - INFO - Iter [2050/160000]	lr: 9.886e-03, eta: 1 day, 16:18:00, time: 0.845, data_time: 0.010, memory: 4496, decode.loss_seg: 1.8411, decode.acc_seg: 37.5875, loss: 1.8411
2021-08-13 11:43:57,439 - mmseg - INFO - Iter [2100/160000]	lr: 9.883e-03, eta: 1 day, 16:14:21, time: 0.873, data_time: 0.010, memory: 4496, decode.loss_seg: 1.9408, decode.acc_seg: 35.9524, loss: 1.9408
2021-08-13 11:44:39,927 - mmseg - INFO - Iter [2150/160000]	lr: 9.880e-03, eta: 1 day, 16:09:29, time: 0.850, data_time: 0.010, memory: 4496, decode.loss_seg: 1.9122, decode.acc_seg: 35.3518, loss: 1.9122
2021-08-13 11:45:22,447 - mmseg - INFO - Iter [2200/160000]	lr: 9.877e-03, eta: 1 day, 16:04:48, time: 0.850, data_time: 0.009, memory: 4496, decode.loss_seg: 1.8357, decode.acc_seg: 36.7165, loss: 1.8357
2021-08-13 11:46:05,527 - mmseg - INFO - Iter [2250/160000]	lr: 9.875e-03, eta: 1 day, 16:00:56, time: 0.861, data_time: 0.010, memory: 4496, decode.loss_seg: 1.8770, decode.acc_seg: 36.7111, loss: 1.8770
2021-08-13 11:46:47,781 - mmseg - INFO - Iter [2300/160000]	lr: 9.872e-03, eta: 1 day, 15:56:18, time: 0.845, data_time: 0.010, memory: 4496, decode.loss_seg: 1.8905, decode.acc_seg: 36.2069, loss: 1.8905
2021-08-13 11:47:32,719 - mmseg - INFO - Iter [2350/160000]	lr: 9.869e-03, eta: 1 day, 15:54:47, time: 0.898, data_time: 0.009, memory: 4496, decode.loss_seg: 1.8762, decode.acc_seg: 37.0006, loss: 1.8762
2021-08-13 11:48:16,605 - mmseg - INFO - Iter [2400/160000]	lr: 9.866e-03, eta: 1 day, 15:52:13, time: 0.878, data_time: 0.010, memory: 4496, decode.loss_seg: 1.8151, decode.acc_seg: 38.8103, loss: 1.8151
2021-08-13 11:48:58,649 - mmseg - INFO - Iter [2450/160000]	lr: 9.864e-03, eta: 1 day, 15:47:42, time: 0.841, data_time: 0.010, memory: 4496, decode.loss_seg: 1.8064, decode.acc_seg: 38.2554, loss: 1.8064
2021-08-13 11:49:39,967 - mmseg - INFO - Iter [2500/160000]	lr: 9.861e-03, eta: 1 day, 15:42:36, time: 0.827, data_time: 0.010, memory: 4496, decode.loss_seg: 1.8856, decode.acc_seg: 37.1347, loss: 1.8856
2021-08-13 11:50:59,120 - mmseg - INFO - Iter [2550/160000]	lr: 9.858e-03, eta: 1 day, 16:16:34, time: 1.582, data_time: 0.700, memory: 4496, decode.loss_seg: 1.8049, decode.acc_seg: 37.7571, loss: 1.8049
2021-08-13 11:51:43,193 - mmseg - INFO - Iter [2600/160000]	lr: 9.855e-03, eta: 1 day, 16:13:48, time: 0.881, data_time: 0.010, memory: 4496, decode.loss_seg: 1.7733, decode.acc_seg: 38.0748, loss: 1.7733
2021-08-13 11:52:27,444 - mmseg - INFO - Iter [2650/160000]	lr: 9.852e-03, eta: 1 day, 16:11:20, time: 0.886, data_time: 0.010, memory: 4496, decode.loss_seg: 1.8552, decode.acc_seg: 37.9913, loss: 1.8552
2021-08-13 11:53:10,577 - mmseg - INFO - Iter [2700/160000]	lr: 9.850e-03, eta: 1 day, 16:07:47, time: 0.862, data_time: 0.010, memory: 4496, decode.loss_seg: 1.8252, decode.acc_seg: 38.4322, loss: 1.8252
2021-08-13 11:53:53,182 - mmseg - INFO - Iter [2750/160000]	lr: 9.847e-03, eta: 1 day, 16:03:51, time: 0.852, data_time: 0.010, memory: 4496, decode.loss_seg: 1.8372, decode.acc_seg: 37.1362, loss: 1.8372
2021-08-13 11:54:37,513 - mmseg - INFO - Iter [2800/160000]	lr: 9.844e-03, eta: 1 day, 16:01:40, time: 0.887, data_time: 0.010, memory: 4496, decode.loss_seg: 1.7265, decode.acc_seg: 38.8918, loss: 1.7265
2021-08-13 11:55:19,913 - mmseg - INFO - Iter [2850/160000]	lr: 9.841e-03, eta: 1 day, 15:57:46, time: 0.848, data_time: 0.010, memory: 4496, decode.loss_seg: 1.7982, decode.acc_seg: 37.9417, loss: 1.7982
2021-08-13 11:56:03,055 - mmseg - INFO - Iter [2900/160000]	lr: 9.838e-03, eta: 1 day, 15:54:37, time: 0.863, data_time: 0.010, memory: 4496, decode.loss_seg: 1.7386, decode.acc_seg: 39.3367, loss: 1.7386
2021-08-13 11:56:45,416 - mmseg - INFO - Iter [2950/160000]	lr: 9.836e-03, eta: 1 day, 15:50:53, time: 0.847, data_time: 0.009, memory: 4496, decode.loss_seg: 1.7569, decode.acc_seg: 39.2402, loss: 1.7569
2021-08-13 11:57:27,764 - mmseg - INFO - Exp name: fcn_litehr18-with-head_512x512_160k_ade20k.py
2021-08-13 11:57:27,765 - mmseg - INFO - Iter [3000/160000]	lr: 9.833e-03, eta: 1 day, 15:47:12, time: 0.846, data_time: 0.009, memory: 4496, decode.loss_seg: 1.7988, decode.acc_seg: 38.2520, loss: 1.7988
2021-08-13 11:58:11,829 - mmseg - INFO - Iter [3050/160000]	lr: 9.830e-03, eta: 1 day, 15:45:06, time: 0.881, data_time: 0.010, memory: 4496, decode.loss_seg: 1.8058, decode.acc_seg: 38.5455, loss: 1.8058
2021-08-13 11:58:57,240 - mmseg - INFO - Iter [3100/160000]	lr: 9.827e-03, eta: 1 day, 15:44:11, time: 0.908, data_time: 0.010, memory: 4496, decode.loss_seg: 1.7742, decode.acc_seg: 39.0588, loss: 1.7742
2021-08-13 11:59:40,950 - mmseg - INFO - Iter [3150/160000]	lr: 9.824e-03, eta: 1 day, 15:41:53, time: 0.875, data_time: 0.010, memory: 4496, decode.loss_seg: 1.7689, decode.acc_seg: 39.5991, loss: 1.7689
2021-08-13 12:01:44,462 - mmseg - INFO - Iter [3200/160000]	lr: 9.822e-03, eta: 1 day, 16:44:46, time: 2.470, data_time: 1.600, memory: 4496, decode.loss_seg: 1.7876, decode.acc_seg: 37.9272, loss: 1.7876
2021-08-13 12:02:27,504 - mmseg - INFO - Iter [3250/160000]	lr: 9.819e-03, eta: 1 day, 16:41:00, time: 0.861, data_time: 0.011, memory: 4496, decode.loss_seg: 1.7904, decode.acc_seg: 39.7776, loss: 1.7904
2021-08-13 12:03:10,846 - mmseg - INFO - Iter [3300/160000]	lr: 9.816e-03, eta: 1 day, 16:37:33, time: 0.866, data_time: 0.010, memory: 4496, decode.loss_seg: 1.7720, decode.acc_seg: 39.2342, loss: 1.7720
2021-08-13 12:03:54,001 - mmseg - INFO - Iter [3350/160000]	lr: 9.813e-03, eta: 1 day, 16:34:02, time: 0.863, data_time: 0.010, memory: 4496, decode.loss_seg: 1.7628, decode.acc_seg: 39.3881, loss: 1.7628
2021-08-13 12:04:37,245 - mmseg - INFO - Iter [3400/160000]	lr: 9.811e-03, eta: 1 day, 16:30:41, time: 0.865, data_time: 0.010, memory: 4496, decode.loss_seg: 1.7395, decode.acc_seg: 39.8627, loss: 1.7395
2021-08-13 12:05:21,249 - mmseg - INFO - Iter [3450/160000]	lr: 9.808e-03, eta: 1 day, 16:27:57, time: 0.880, data_time: 0.010, memory: 4496, decode.loss_seg: 1.7591, decode.acc_seg: 39.0613, loss: 1.7591
2021-08-13 12:06:05,576 - mmseg - INFO - Iter [3500/160000]	lr: 9.805e-03, eta: 1 day, 16:25:32, time: 0.886, data_time: 0.010, memory: 4496, decode.loss_seg: 1.7429, decode.acc_seg: 39.2784, loss: 1.7429
2021-08-13 12:06:49,313 - mmseg - INFO - Iter [3550/160000]	lr: 9.802e-03, eta: 1 day, 16:22:44, time: 0.875, data_time: 0.010, memory: 4496, decode.loss_seg: 1.7175, decode.acc_seg: 38.9839, loss: 1.7175
2021-08-13 12:07:31,574 - mmseg - INFO - Iter [3600/160000]	lr: 9.799e-03, eta: 1 day, 16:18:56, time: 0.846, data_time: 0.010, memory: 4496, decode.loss_seg: 1.7252, decode.acc_seg: 40.2543, loss: 1.7252
2021-08-13 12:08:15,994 - mmseg - INFO - Iter [3650/160000]	lr: 9.797e-03, eta: 1 day, 16:16:44, time: 0.888, data_time: 0.010, memory: 4496, decode.loss_seg: 1.7185, decode.acc_seg: 40.0359, loss: 1.7185
2021-08-13 12:09:00,053 - mmseg - INFO - Iter [3700/160000]	lr: 9.794e-03, eta: 1 day, 16:14:20, time: 0.882, data_time: 0.011, memory: 4496, decode.loss_seg: 1.7005, decode.acc_seg: 40.3620, loss: 1.7005
2021-08-13 12:09:45,324 - mmseg - INFO - Iter [3750/160000]	lr: 9.791e-03, eta: 1 day, 16:12:49, time: 0.905, data_time: 0.010, memory: 4496, decode.loss_seg: 1.6953, decode.acc_seg: 40.3500, loss: 1.6953
2021-08-13 12:11:07,451 - mmseg - INFO - Iter [3800/160000]	lr: 9.788e-03, eta: 1 day, 16:36:35, time: 1.643, data_time: 0.763, memory: 4496, decode.loss_seg: 1.7466, decode.acc_seg: 39.7114, loss: 1.7466
2021-08-13 12:11:50,468 - mmseg - INFO - Iter [3850/160000]	lr: 9.785e-03, eta: 1 day, 16:33:14, time: 0.860, data_time: 0.010, memory: 4496, decode.loss_seg: 1.7075, decode.acc_seg: 39.8478, loss: 1.7075
2021-08-13 12:12:33,736 - mmseg - INFO - Iter [3900/160000]	lr: 9.783e-03, eta: 1 day, 16:30:09, time: 0.866, data_time: 0.010, memory: 4496, decode.loss_seg: 1.6548, decode.acc_seg: 40.8692, loss: 1.6548
2021-08-13 12:13:15,990 - mmseg - INFO - Iter [3950/160000]	lr: 9.780e-03, eta: 1 day, 16:26:25, time: 0.845, data_time: 0.010, memory: 4496, decode.loss_seg: 1.7351, decode.acc_seg: 40.2292, loss: 1.7351
2021-08-13 12:13:59,207 - mmseg - INFO - Exp name: fcn_litehr18-with-head_512x512_160k_ade20k.py
2021-08-13 12:13:59,207 - mmseg - INFO - Iter [4000/160000]	lr: 9.777e-03, eta: 1 day, 16:23:26, time: 0.865, data_time: 0.010, memory: 4496, decode.loss_seg: 1.6825, decode.acc_seg: 40.6703, loss: 1.6825
2021-08-13 12:14:42,110 - mmseg - INFO - Iter [4050/160000]	lr: 9.774e-03, eta: 1 day, 16:20:16, time: 0.858, data_time: 0.010, memory: 4496, decode.loss_seg: 1.7004, decode.acc_seg: 40.4774, loss: 1.7004
2021-08-13 12:15:24,661 - mmseg - INFO - Iter [4100/160000]	lr: 9.771e-03, eta: 1 day, 16:16:57, time: 0.851, data_time: 0.011, memory: 4496, decode.loss_seg: 1.6996, decode.acc_seg: 40.0663, loss: 1.6996
2021-08-13 12:16:07,941 - mmseg - INFO - Iter [4150/160000]	lr: 9.769e-03, eta: 1 day, 16:14:11, time: 0.866, data_time: 0.010, memory: 4496, decode.loss_seg: 1.7053, decode.acc_seg: 39.7016, loss: 1.7053
2021-08-13 12:16:50,690 - mmseg - INFO - Iter [4200/160000]	lr: 9.766e-03, eta: 1 day, 16:11:05, time: 0.854, data_time: 0.010, memory: 4496, decode.loss_seg: 1.6905, decode.acc_seg: 39.9644, loss: 1.6905
2021-08-13 12:17:32,774 - mmseg - INFO - Iter [4250/160000]	lr: 9.763e-03, eta: 1 day, 16:07:40, time: 0.842, data_time: 0.010, memory: 4496, decode.loss_seg: 1.6675, decode.acc_seg: 41.0826, loss: 1.6675
2021-08-13 12:18:17,006 - mmseg - INFO - Iter [4300/160000]	lr: 9.760e-03, eta: 1 day, 16:05:34, time: 0.884, data_time: 0.009, memory: 4496, decode.loss_seg: 1.6623, decode.acc_seg: 40.7241, loss: 1.6623
2021-08-13 12:19:00,503 - mmseg - INFO - Iter [4350/160000]	lr: 9.757e-03, eta: 1 day, 16:03:07, time: 0.871, data_time: 0.011, memory: 4496, decode.loss_seg: 1.6884, decode.acc_seg: 40.7415, loss: 1.6884
2021-08-13 12:19:42,609 - mmseg - INFO - Iter [4400/160000]	lr: 9.755e-03, eta: 1 day, 15:59:52, time: 0.842, data_time: 0.010, memory: 4496, decode.loss_seg: 1.6775, decode.acc_seg: 40.5998, loss: 1.6775
2021-08-13 12:20:59,653 - mmseg - INFO - Iter [4450/160000]	lr: 9.752e-03, eta: 1 day, 16:17:01, time: 1.541, data_time: 0.660, memory: 4496, decode.loss_seg: 1.6440, decode.acc_seg: 41.5029, loss: 1.6440
2021-08-13 12:21:43,857 - mmseg - INFO - Iter [4500/160000]	lr: 9.749e-03, eta: 1 day, 16:14:50, time: 0.884, data_time: 0.011, memory: 4496, decode.loss_seg: 1.6594, decode.acc_seg: 40.1489, loss: 1.6594
2021-08-13 12:22:29,046 - mmseg - INFO - Iter [4550/160000]	lr: 9.746e-03, eta: 1 day, 16:13:17, time: 0.904, data_time: 0.011, memory: 4496, decode.loss_seg: 1.7124, decode.acc_seg: 39.9604, loss: 1.7124
2021-08-13 12:23:11,885 - mmseg - INFO - Iter [4600/160000]	lr: 9.744e-03, eta: 1 day, 16:10:24, time: 0.857, data_time: 0.010, memory: 4496, decode.loss_seg: 1.6845, decode.acc_seg: 41.2544, loss: 1.6845
2021-08-13 12:23:55,821 - mmseg - INFO - Iter [4650/160000]	lr: 9.741e-03, eta: 1 day, 16:08:11, time: 0.879, data_time: 0.011, memory: 4496, decode.loss_seg: 1.5912, decode.acc_seg: 42.0982, loss: 1.5912
2021-08-13 12:24:38,857 - mmseg - INFO - Iter [4700/160000]	lr: 9.738e-03, eta: 1 day, 16:05:29, time: 0.861, data_time: 0.010, memory: 4496, decode.loss_seg: 1.6100, decode.acc_seg: 41.4716, loss: 1.6100
2021-08-13 12:25:21,596 - mmseg - INFO - Iter [4750/160000]	lr: 9.735e-03, eta: 1 day, 16:02:41, time: 0.855, data_time: 0.010, memory: 4496, decode.loss_seg: 1.6390, decode.acc_seg: 40.7520, loss: 1.6390
2021-08-13 12:26:04,997 - mmseg - INFO - Iter [4800/160000]	lr: 9.732e-03, eta: 1 day, 16:00:17, time: 0.868, data_time: 0.010, memory: 4496, decode.loss_seg: 1.6213, decode.acc_seg: 41.8408, loss: 1.6213
2021-08-13 12:26:48,193 - mmseg - INFO - Iter [4850/160000]	lr: 9.730e-03, eta: 1 day, 15:57:49, time: 0.864, data_time: 0.010, memory: 4496, decode.loss_seg: 1.6616, decode.acc_seg: 41.5739, loss: 1.6616
2021-08-13 12:27:31,133 - mmseg - INFO - Iter [4900/160000]	lr: 9.727e-03, eta: 1 day, 15:55:13, time: 0.858, data_time: 0.009, memory: 4496, decode.loss_seg: 1.6100, decode.acc_seg: 43.1307, loss: 1.6100
2021-08-13 12:28:15,896 - mmseg - INFO - Iter [4950/160000]	lr: 9.724e-03, eta: 1 day, 15:53:37, time: 0.895, data_time: 0.010, memory: 4496, decode.loss_seg: 1.6854, decode.acc_seg: 40.4854, loss: 1.6854
2021-08-13 12:28:59,255 - mmseg - INFO - Exp name: fcn_litehr18-with-head_512x512_160k_ade20k.py
2021-08-13 12:28:59,256 - mmseg - INFO - Iter [5000/160000]	lr: 9.721e-03, eta: 1 day, 15:51:21, time: 0.868, data_time: 0.010, memory: 4496, decode.loss_seg: 1.6505, decode.acc_seg: 41.6656, loss: 1.6505
2021-08-13 12:30:16,523 - mmseg - INFO - Iter [5050/160000]	lr: 9.718e-03, eta: 1 day, 16:06:24, time: 1.545, data_time: 0.711, memory: 4496, decode.loss_seg: 1.6377, decode.acc_seg: 41.3185, loss: 1.6377
2021-08-13 12:30:59,634 - mmseg - INFO - Iter [5100/160000]	lr: 9.716e-03, eta: 1 day, 16:03:52, time: 0.862, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5256, decode.acc_seg: 42.5473, loss: 1.5256
2021-08-13 12:31:41,770 - mmseg - INFO - Iter [5150/160000]	lr: 9.713e-03, eta: 1 day, 16:00:53, time: 0.843, data_time: 0.010, memory: 4496, decode.loss_seg: 1.6488, decode.acc_seg: 40.9643, loss: 1.6488
2021-08-13 12:32:24,915 - mmseg - INFO - Iter [5200/160000]	lr: 9.710e-03, eta: 1 day, 15:58:25, time: 0.862, data_time: 0.010, memory: 4496, decode.loss_seg: 1.6363, decode.acc_seg: 41.9563, loss: 1.6363
2021-08-13 12:33:07,885 - mmseg - INFO - Iter [5250/160000]	lr: 9.707e-03, eta: 1 day, 15:55:56, time: 0.860, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5929, decode.acc_seg: 42.8840, loss: 1.5929
2021-08-13 12:33:49,740 - mmseg - INFO - Iter [5300/160000]	lr: 9.704e-03, eta: 1 day, 15:52:56, time: 0.837, data_time: 0.010, memory: 4496, decode.loss_seg: 1.6379, decode.acc_seg: 41.9971, loss: 1.6379
2021-08-13 12:34:32,432 - mmseg - INFO - Iter [5350/160000]	lr: 9.702e-03, eta: 1 day, 15:50:22, time: 0.854, data_time: 0.010, memory: 4496, decode.loss_seg: 1.6617, decode.acc_seg: 41.7221, loss: 1.6617
2021-08-13 12:35:14,775 - mmseg - INFO - Iter [5400/160000]	lr: 9.699e-03, eta: 1 day, 15:47:41, time: 0.847, data_time: 0.010, memory: 4496, decode.loss_seg: 1.6284, decode.acc_seg: 43.3590, loss: 1.6284
2021-08-13 12:35:57,641 - mmseg - INFO - Iter [5450/160000]	lr: 9.696e-03, eta: 1 day, 15:45:15, time: 0.857, data_time: 0.009, memory: 4496, decode.loss_seg: 1.5780, decode.acc_seg: 42.7296, loss: 1.5780
2021-08-13 12:36:40,907 - mmseg - INFO - Iter [5500/160000]	lr: 9.693e-03, eta: 1 day, 15:43:04, time: 0.865, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5393, decode.acc_seg: 43.0013, loss: 1.5393
2021-08-13 12:37:23,303 - mmseg - INFO - Iter [5550/160000]	lr: 9.690e-03, eta: 1 day, 15:40:29, time: 0.848, data_time: 0.009, memory: 4496, decode.loss_seg: 1.6247, decode.acc_seg: 42.3244, loss: 1.6247
2021-08-13 12:38:06,352 - mmseg - INFO - Iter [5600/160000]	lr: 9.688e-03, eta: 1 day, 15:38:15, time: 0.861, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5412, decode.acc_seg: 42.1201, loss: 1.5412
2021-08-13 12:38:49,103 - mmseg - INFO - Iter [5650/160000]	lr: 9.685e-03, eta: 1 day, 15:35:54, time: 0.855, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5823, decode.acc_seg: 41.9047, loss: 1.5823
2021-08-13 12:40:07,600 - mmseg - INFO - Iter [5700/160000]	lr: 9.682e-03, eta: 1 day, 15:49:43, time: 1.570, data_time: 0.711, memory: 4496, decode.loss_seg: 1.6092, decode.acc_seg: 41.5805, loss: 1.6092
2021-08-13 12:40:52,716 - mmseg - INFO - Iter [5750/160000]	lr: 9.679e-03, eta: 1 day, 15:48:20, time: 0.902, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5882, decode.acc_seg: 41.4787, loss: 1.5882
2021-08-13 12:41:35,760 - mmseg - INFO - Iter [5800/160000]	lr: 9.676e-03, eta: 1 day, 15:46:03, time: 0.861, data_time: 0.010, memory: 4496, decode.loss_seg: 1.6046, decode.acc_seg: 42.9081, loss: 1.6046
2021-08-13 12:42:18,250 - mmseg - INFO - Iter [5850/160000]	lr: 9.674e-03, eta: 1 day, 15:43:34, time: 0.850, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5865, decode.acc_seg: 42.0744, loss: 1.5865
2021-08-13 12:43:00,540 - mmseg - INFO - Iter [5900/160000]	lr: 9.671e-03, eta: 1 day, 15:41:01, time: 0.846, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5997, decode.acc_seg: 42.2412, loss: 1.5997
2021-08-13 12:43:43,948 - mmseg - INFO - Iter [5950/160000]	lr: 9.668e-03, eta: 1 day, 15:38:58, time: 0.868, data_time: 0.009, memory: 4496, decode.loss_seg: 1.5968, decode.acc_seg: 41.5840, loss: 1.5968
2021-08-13 12:44:25,699 - mmseg - INFO - Exp name: fcn_litehr18-with-head_512x512_160k_ade20k.py
2021-08-13 12:44:25,699 - mmseg - INFO - Iter [6000/160000]	lr: 9.665e-03, eta: 1 day, 15:36:14, time: 0.835, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5345, decode.acc_seg: 42.5579, loss: 1.5345
2021-08-13 12:45:07,165 - mmseg - INFO - Iter [6050/160000]	lr: 9.663e-03, eta: 1 day, 15:33:25, time: 0.829, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5629, decode.acc_seg: 43.2652, loss: 1.5629
2021-08-13 12:45:49,440 - mmseg - INFO - Iter [6100/160000]	lr: 9.660e-03, eta: 1 day, 15:30:58, time: 0.845, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5875, decode.acc_seg: 42.5058, loss: 1.5875
2021-08-13 12:46:32,295 - mmseg - INFO - Iter [6150/160000]	lr: 9.657e-03, eta: 1 day, 15:28:48, time: 0.857, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5674, decode.acc_seg: 44.5341, loss: 1.5674
2021-08-13 12:47:14,678 - mmseg - INFO - Iter [6200/160000]	lr: 9.654e-03, eta: 1 day, 15:26:27, time: 0.847, data_time: 0.009, memory: 4496, decode.loss_seg: 1.5583, decode.acc_seg: 43.7786, loss: 1.5583
2021-08-13 12:47:59,674 - mmseg - INFO - Iter [6250/160000]	lr: 9.651e-03, eta: 1 day, 15:25:12, time: 0.900, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5930, decode.acc_seg: 42.1770, loss: 1.5930
2021-08-13 12:48:44,252 - mmseg - INFO - Iter [6300/160000]	lr: 9.649e-03, eta: 1 day, 15:23:48, time: 0.892, data_time: 0.010, memory: 4496, decode.loss_seg: 1.6375, decode.acc_seg: 42.3829, loss: 1.6375
2021-08-13 12:50:01,332 - mmseg - INFO - Iter [6350/160000]	lr: 9.646e-03, eta: 1 day, 15:35:30, time: 1.541, data_time: 0.713, memory: 4496, decode.loss_seg: 1.6058, decode.acc_seg: 43.3977, loss: 1.6058
2021-08-13 12:50:44,651 - mmseg - INFO - Iter [6400/160000]	lr: 9.643e-03, eta: 1 day, 15:33:30, time: 0.866, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5337, decode.acc_seg: 43.6868, loss: 1.5337
2021-08-13 12:51:28,264 - mmseg - INFO - Iter [6450/160000]	lr: 9.640e-03, eta: 1 day, 15:31:39, time: 0.873, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5506, decode.acc_seg: 42.5570, loss: 1.5506
2021-08-13 12:52:11,632 - mmseg - INFO - Iter [6500/160000]	lr: 9.637e-03, eta: 1 day, 15:29:42, time: 0.867, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5945, decode.acc_seg: 42.0826, loss: 1.5945
2021-08-13 12:52:54,375 - mmseg - INFO - Iter [6550/160000]	lr: 9.635e-03, eta: 1 day, 15:27:32, time: 0.854, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5680, decode.acc_seg: 42.3174, loss: 1.5680
2021-08-13 12:53:36,136 - mmseg - INFO - Iter [6600/160000]	lr: 9.632e-03, eta: 1 day, 15:25:01, time: 0.836, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5461, decode.acc_seg: 42.9420, loss: 1.5461
2021-08-13 12:54:18,477 - mmseg - INFO - Iter [6650/160000]	lr: 9.629e-03, eta: 1 day, 15:22:44, time: 0.847, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5695, decode.acc_seg: 43.0329, loss: 1.5695
2021-08-13 12:54:59,804 - mmseg - INFO - Iter [6700/160000]	lr: 9.626e-03, eta: 1 day, 15:20:06, time: 0.827, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5397, decode.acc_seg: 44.3484, loss: 1.5397
2021-08-13 12:55:41,822 - mmseg - INFO - Iter [6750/160000]	lr: 9.623e-03, eta: 1 day, 15:17:45, time: 0.840, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5499, decode.acc_seg: 42.8055, loss: 1.5499
2021-08-13 12:56:23,564 - mmseg - INFO - Iter [6800/160000]	lr: 9.621e-03, eta: 1 day, 15:15:20, time: 0.835, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5744, decode.acc_seg: 43.6523, loss: 1.5744
2021-08-13 12:57:06,671 - mmseg - INFO - Iter [6850/160000]	lr: 9.618e-03, eta: 1 day, 15:13:26, time: 0.862, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5690, decode.acc_seg: 43.4355, loss: 1.5690
2021-08-13 12:57:50,028 - mmseg - INFO - Iter [6900/160000]	lr: 9.615e-03, eta: 1 day, 15:11:39, time: 0.867, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5564, decode.acc_seg: 44.2209, loss: 1.5564
2021-08-13 12:59:08,259 - mmseg - INFO - Iter [6950/160000]	lr: 9.612e-03, eta: 1 day, 15:22:41, time: 1.565, data_time: 0.677, memory: 4496, decode.loss_seg: 1.4922, decode.acc_seg: 43.6768, loss: 1.4922
2021-08-13 12:59:50,382 - mmseg - INFO - Exp name: fcn_litehr18-with-head_512x512_160k_ade20k.py
2021-08-13 12:59:50,383 - mmseg - INFO - Iter [7000/160000]	lr: 9.609e-03, eta: 1 day, 15:20:23, time: 0.842, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5440, decode.acc_seg: 43.1391, loss: 1.5440
2021-08-13 13:00:33,125 - mmseg - INFO - Iter [7050/160000]	lr: 9.607e-03, eta: 1 day, 15:18:20, time: 0.855, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5535, decode.acc_seg: 43.7226, loss: 1.5535
2021-08-13 13:01:16,149 - mmseg - INFO - Iter [7100/160000]	lr: 9.604e-03, eta: 1 day, 15:16:24, time: 0.860, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5474, decode.acc_seg: 42.7790, loss: 1.5474
2021-08-13 13:01:59,148 - mmseg - INFO - Iter [7150/160000]	lr: 9.601e-03, eta: 1 day, 15:14:29, time: 0.860, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5453, decode.acc_seg: 43.5740, loss: 1.5453
2021-08-13 13:02:40,387 - mmseg - INFO - Iter [7200/160000]	lr: 9.598e-03, eta: 1 day, 15:11:57, time: 0.825, data_time: 0.009, memory: 4496, decode.loss_seg: 1.4812, decode.acc_seg: 43.8780, loss: 1.4812
2021-08-13 13:03:23,877 - mmseg - INFO - Iter [7250/160000]	lr: 9.595e-03, eta: 1 day, 15:10:14, time: 0.869, data_time: 0.009, memory: 4496, decode.loss_seg: 1.5173, decode.acc_seg: 44.4506, loss: 1.5173
2021-08-13 13:04:09,414 - mmseg - INFO - Iter [7300/160000]	lr: 9.593e-03, eta: 1 day, 15:09:14, time: 0.911, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5576, decode.acc_seg: 43.5936, loss: 1.5576
2021-08-13 13:04:53,815 - mmseg - INFO - Iter [7350/160000]	lr: 9.590e-03, eta: 1 day, 15:07:53, time: 0.889, data_time: 0.011, memory: 4496, decode.loss_seg: 1.5298, decode.acc_seg: 43.8199, loss: 1.5298
2021-08-13 13:05:35,833 - mmseg - INFO - Iter [7400/160000]	lr: 9.587e-03, eta: 1 day, 15:05:41, time: 0.840, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4783, decode.acc_seg: 44.2235, loss: 1.4783
2021-08-13 13:06:18,884 - mmseg - INFO - Iter [7450/160000]	lr: 9.584e-03, eta: 1 day, 15:03:52, time: 0.860, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5364, decode.acc_seg: 44.8144, loss: 1.5364
2021-08-13 13:07:04,096 - mmseg - INFO - Iter [7500/160000]	lr: 9.581e-03, eta: 1 day, 15:02:48, time: 0.904, data_time: 0.010, memory: 4496, decode.loss_seg: 1.6059, decode.acc_seg: 43.1341, loss: 1.6059
2021-08-13 13:07:47,237 - mmseg - INFO - Iter [7550/160000]	lr: 9.579e-03, eta: 1 day, 15:01:03, time: 0.863, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5228, decode.acc_seg: 43.9541, loss: 1.5228
2021-08-13 13:09:05,280 - mmseg - INFO - Iter [7600/160000]	lr: 9.576e-03, eta: 1 day, 15:10:58, time: 1.561, data_time: 0.712, memory: 4496, decode.loss_seg: 1.4811, decode.acc_seg: 44.1457, loss: 1.4811
2021-08-13 13:09:47,780 - mmseg - INFO - Iter [7650/160000]	lr: 9.573e-03, eta: 1 day, 15:08:56, time: 0.850, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5306, decode.acc_seg: 43.5831, loss: 1.5306
2021-08-13 13:10:30,890 - mmseg - INFO - Iter [7700/160000]	lr: 9.570e-03, eta: 1 day, 15:07:08, time: 0.862, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4958, decode.acc_seg: 44.8083, loss: 1.4958
2021-08-13 13:11:12,782 - mmseg - INFO - Iter [7750/160000]	lr: 9.567e-03, eta: 1 day, 15:04:56, time: 0.838, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5060, decode.acc_seg: 43.9007, loss: 1.5060
2021-08-13 13:11:55,517 - mmseg - INFO - Iter [7800/160000]	lr: 9.565e-03, eta: 1 day, 15:03:02, time: 0.855, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5346, decode.acc_seg: 43.5498, loss: 1.5346
2021-08-13 13:12:37,859 - mmseg - INFO - Iter [7850/160000]	lr: 9.562e-03, eta: 1 day, 15:01:02, time: 0.847, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5017, decode.acc_seg: 43.9048, loss: 1.5017
2021-08-13 13:13:21,273 - mmseg - INFO - Iter [7900/160000]	lr: 9.559e-03, eta: 1 day, 14:59:23, time: 0.868, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5319, decode.acc_seg: 43.5345, loss: 1.5319
2021-08-13 13:14:05,138 - mmseg - INFO - Iter [7950/160000]	lr: 9.556e-03, eta: 1 day, 14:57:53, time: 0.877, data_time: 0.009, memory: 4496, decode.loss_seg: 1.5086, decode.acc_seg: 43.9567, loss: 1.5086
2021-08-13 13:14:48,252 - mmseg - INFO - Exp name: fcn_litehr18-with-head_512x512_160k_ade20k.py
2021-08-13 13:14:48,253 - mmseg - INFO - Iter [8000/160000]	lr: 9.553e-03, eta: 1 day, 14:56:10, time: 0.863, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5243, decode.acc_seg: 43.5904, loss: 1.5243
2021-08-13 13:15:31,204 - mmseg - INFO - Iter [8050/160000]	lr: 9.551e-03, eta: 1 day, 14:54:24, time: 0.859, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4808, decode.acc_seg: 44.2952, loss: 1.4808
2021-08-13 13:16:15,030 - mmseg - INFO - Iter [8100/160000]	lr: 9.548e-03, eta: 1 day, 14:52:55, time: 0.877, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5236, decode.acc_seg: 44.5721, loss: 1.5236
2021-08-13 13:16:57,875 - mmseg - INFO - Iter [8150/160000]	lr: 9.545e-03, eta: 1 day, 14:51:09, time: 0.857, data_time: 0.011, memory: 4496, decode.loss_seg: 1.5271, decode.acc_seg: 44.2641, loss: 1.5271
2021-08-13 13:17:41,091 - mmseg - INFO - Iter [8200/160000]	lr: 9.542e-03, eta: 1 day, 14:49:30, time: 0.863, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5102, decode.acc_seg: 43.4915, loss: 1.5102
2021-08-13 13:18:58,266 - mmseg - INFO - Iter [8250/160000]	lr: 9.539e-03, eta: 1 day, 14:58:17, time: 1.544, data_time: 0.655, memory: 4496, decode.loss_seg: 1.4666, decode.acc_seg: 44.5068, loss: 1.4666
2021-08-13 13:19:41,174 - mmseg - INFO - Iter [8300/160000]	lr: 9.537e-03, eta: 1 day, 14:56:31, time: 0.859, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5237, decode.acc_seg: 43.9540, loss: 1.5237
2021-08-13 13:20:22,759 - mmseg - INFO - Iter [8350/160000]	lr: 9.534e-03, eta: 1 day, 14:54:20, time: 0.831, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5194, decode.acc_seg: 44.5136, loss: 1.5194
2021-08-13 13:21:05,154 - mmseg - INFO - Iter [8400/160000]	lr: 9.531e-03, eta: 1 day, 14:52:26, time: 0.848, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4772, decode.acc_seg: 44.5144, loss: 1.4772
2021-08-13 13:21:47,187 - mmseg - INFO - Iter [8450/160000]	lr: 9.528e-03, eta: 1 day, 14:50:26, time: 0.841, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4839, decode.acc_seg: 44.3999, loss: 1.4839
2021-08-13 13:22:29,660 - mmseg - INFO - Iter [8500/160000]	lr: 9.525e-03, eta: 1 day, 14:48:34, time: 0.849, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5008, decode.acc_seg: 43.8391, loss: 1.5008
2021-08-13 13:23:13,461 - mmseg - INFO - Iter [8550/160000]	lr: 9.523e-03, eta: 1 day, 14:47:07, time: 0.876, data_time: 0.011, memory: 4496, decode.loss_seg: 1.5153, decode.acc_seg: 43.9888, loss: 1.5153
2021-08-13 13:23:55,966 - mmseg - INFO - Iter [8600/160000]	lr: 9.520e-03, eta: 1 day, 14:45:18, time: 0.851, data_time: 0.011, memory: 4496, decode.loss_seg: 1.4808, decode.acc_seg: 44.6794, loss: 1.4808
2021-08-13 13:24:38,076 - mmseg - INFO - Iter [8650/160000]	lr: 9.517e-03, eta: 1 day, 14:43:22, time: 0.842, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5070, decode.acc_seg: 44.5297, loss: 1.5070
2021-08-13 13:25:21,057 - mmseg - INFO - Iter [8700/160000]	lr: 9.514e-03, eta: 1 day, 14:41:43, time: 0.860, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4967, decode.acc_seg: 44.4870, loss: 1.4967
2021-08-13 13:26:02,953 - mmseg - INFO - Iter [8750/160000]	lr: 9.511e-03, eta: 1 day, 14:39:46, time: 0.838, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4760, decode.acc_seg: 44.4296, loss: 1.4760
2021-08-13 13:26:45,298 - mmseg - INFO - Iter [8800/160000]	lr: 9.509e-03, eta: 1 day, 14:37:56, time: 0.847, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4810, decode.acc_seg: 44.0164, loss: 1.4810
2021-08-13 13:28:03,910 - mmseg - INFO - Iter [8850/160000]	lr: 9.506e-03, eta: 1 day, 14:46:27, time: 1.572, data_time: 0.715, memory: 4496, decode.loss_seg: 1.5056, decode.acc_seg: 44.0690, loss: 1.5056
2021-08-13 13:28:45,824 - mmseg - INFO - Iter [8900/160000]	lr: 9.503e-03, eta: 1 day, 14:44:29, time: 0.838, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4594, decode.acc_seg: 44.9219, loss: 1.4594
2021-08-13 13:29:28,091 - mmseg - INFO - Iter [8950/160000]	lr: 9.500e-03, eta: 1 day, 14:42:37, time: 0.845, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4678, decode.acc_seg: 44.6893, loss: 1.4678
2021-08-13 13:30:12,030 - mmseg - INFO - Exp name: fcn_litehr18-with-head_512x512_160k_ade20k.py
2021-08-13 13:30:12,031 - mmseg - INFO - Iter [9000/160000]	lr: 9.497e-03, eta: 1 day, 14:41:14, time: 0.878, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4931, decode.acc_seg: 44.4151, loss: 1.4931
2021-08-13 13:30:54,670 - mmseg - INFO - Iter [9050/160000]	lr: 9.495e-03, eta: 1 day, 14:39:30, time: 0.853, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4742, decode.acc_seg: 44.8102, loss: 1.4742
2021-08-13 13:31:36,658 - mmseg - INFO - Iter [9100/160000]	lr: 9.492e-03, eta: 1 day, 14:37:36, time: 0.840, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4592, decode.acc_seg: 44.9383, loss: 1.4592
2021-08-13 13:32:18,939 - mmseg - INFO - Iter [9150/160000]	lr: 9.489e-03, eta: 1 day, 14:35:47, time: 0.846, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4854, decode.acc_seg: 45.3553, loss: 1.4854
2021-08-13 13:33:02,074 - mmseg - INFO - Iter [9200/160000]	lr: 9.486e-03, eta: 1 day, 14:34:13, time: 0.862, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4934, decode.acc_seg: 44.1375, loss: 1.4934
2021-08-13 13:33:45,068 - mmseg - INFO - Iter [9250/160000]	lr: 9.483e-03, eta: 1 day, 14:32:38, time: 0.860, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4152, decode.acc_seg: 44.8054, loss: 1.4152
2021-08-13 13:34:28,065 - mmseg - INFO - Iter [9300/160000]	lr: 9.481e-03, eta: 1 day, 14:31:02, time: 0.860, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4666, decode.acc_seg: 44.8481, loss: 1.4666
2021-08-13 13:35:10,179 - mmseg - INFO - Iter [9350/160000]	lr: 9.478e-03, eta: 1 day, 14:29:14, time: 0.843, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5179, decode.acc_seg: 44.7989, loss: 1.5179
2021-08-13 13:35:53,012 - mmseg - INFO - Iter [9400/160000]	lr: 9.475e-03, eta: 1 day, 14:27:37, time: 0.856, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4575, decode.acc_seg: 44.8564, loss: 1.4575
2021-08-13 13:36:35,595 - mmseg - INFO - Iter [9450/160000]	lr: 9.472e-03, eta: 1 day, 14:25:57, time: 0.852, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5039, decode.acc_seg: 43.4555, loss: 1.5039
2021-08-13 13:37:53,301 - mmseg - INFO - Iter [9500/160000]	lr: 9.469e-03, eta: 1 day, 14:33:35, time: 1.554, data_time: 0.657, memory: 4496, decode.loss_seg: 1.4189, decode.acc_seg: 46.4294, loss: 1.4189
2021-08-13 13:38:35,433 - mmseg - INFO - Iter [9550/160000]	lr: 9.467e-03, eta: 1 day, 14:31:46, time: 0.843, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4659, decode.acc_seg: 44.1703, loss: 1.4659
2021-08-13 13:39:18,427 - mmseg - INFO - Iter [9600/160000]	lr: 9.464e-03, eta: 1 day, 14:30:11, time: 0.860, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4800, decode.acc_seg: 44.9883, loss: 1.4800
2021-08-13 13:40:01,469 - mmseg - INFO - Iter [9650/160000]	lr: 9.461e-03, eta: 1 day, 14:28:37, time: 0.860, data_time: 0.010, memory: 4496, decode.loss_seg: 1.5004, decode.acc_seg: 44.8436, loss: 1.5004
2021-08-13 13:40:43,970 - mmseg - INFO - Iter [9700/160000]	lr: 9.458e-03, eta: 1 day, 14:26:56, time: 0.850, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4529, decode.acc_seg: 44.4408, loss: 1.4529
2021-08-13 13:41:26,676 - mmseg - INFO - Iter [9750/160000]	lr: 9.455e-03, eta: 1 day, 14:25:19, time: 0.854, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4176, decode.acc_seg: 46.9594, loss: 1.4176
2021-08-13 13:42:08,978 - mmseg - INFO - Iter [9800/160000]	lr: 9.453e-03, eta: 1 day, 14:23:35, time: 0.846, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4479, decode.acc_seg: 45.0754, loss: 1.4479
2021-08-13 13:42:51,172 - mmseg - INFO - Iter [9850/160000]	lr: 9.450e-03, eta: 1 day, 14:21:51, time: 0.844, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4466, decode.acc_seg: 45.2977, loss: 1.4466
2021-08-13 13:43:33,170 - mmseg - INFO - Iter [9900/160000]	lr: 9.447e-03, eta: 1 day, 14:20:05, time: 0.840, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4588, decode.acc_seg: 46.3099, loss: 1.4588
2021-08-13 13:44:15,915 - mmseg - INFO - Iter [9950/160000]	lr: 9.444e-03, eta: 1 day, 14:18:30, time: 0.855, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4414, decode.acc_seg: 44.1248, loss: 1.4414
2021-08-13 13:44:58,303 - mmseg - INFO - Exp name: fcn_litehr18-with-head_512x512_160k_ade20k.py
2021-08-13 13:44:58,304 - mmseg - INFO - Iter [10000/160000]	lr: 9.441e-03, eta: 1 day, 14:16:51, time: 0.848, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4770, decode.acc_seg: 45.5134, loss: 1.4770
2021-08-13 13:45:40,586 - mmseg - INFO - Iter [10050/160000]	lr: 9.439e-03, eta: 1 day, 14:15:10, time: 0.845, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4411, decode.acc_seg: 45.5211, loss: 1.4411
2021-08-13 13:46:58,102 - mmseg - INFO - Iter [10100/160000]	lr: 9.436e-03, eta: 1 day, 14:22:13, time: 1.551, data_time: 0.719, memory: 4496, decode.loss_seg: 1.4758, decode.acc_seg: 44.6895, loss: 1.4758
2021-08-13 13:47:41,023 - mmseg - INFO - Iter [10150/160000]	lr: 9.433e-03, eta: 1 day, 14:20:40, time: 0.858, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4024, decode.acc_seg: 46.0893, loss: 1.4024
2021-08-13 13:48:25,240 - mmseg - INFO - Iter [10200/160000]	lr: 9.430e-03, eta: 1 day, 14:19:27, time: 0.885, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4081, decode.acc_seg: 46.8369, loss: 1.4081
2021-08-13 13:49:08,532 - mmseg - INFO - Iter [10250/160000]	lr: 9.427e-03, eta: 1 day, 14:18:01, time: 0.865, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4515, decode.acc_seg: 45.5610, loss: 1.4515
2021-08-13 13:49:50,980 - mmseg - INFO - Iter [10300/160000]	lr: 9.425e-03, eta: 1 day, 14:16:23, time: 0.849, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4302, decode.acc_seg: 45.6142, loss: 1.4302
2021-08-13 13:50:32,895 - mmseg - INFO - Iter [10350/160000]	lr: 9.422e-03, eta: 1 day, 14:14:37, time: 0.838, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4553, decode.acc_seg: 45.3640, loss: 1.4553
2021-08-13 13:51:15,749 - mmseg - INFO - Iter [10400/160000]	lr: 9.419e-03, eta: 1 day, 14:13:06, time: 0.857, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4396, decode.acc_seg: 45.2709, loss: 1.4396
2021-08-13 13:51:59,074 - mmseg - INFO - Iter [10450/160000]	lr: 9.416e-03, eta: 1 day, 14:11:42, time: 0.867, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4611, decode.acc_seg: 44.9557, loss: 1.4611
2021-08-13 13:52:41,573 - mmseg - INFO - Iter [10500/160000]	lr: 9.413e-03, eta: 1 day, 14:10:07, time: 0.850, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4419, decode.acc_seg: 45.0000, loss: 1.4419
2021-08-13 13:53:24,820 - mmseg - INFO - Iter [10550/160000]	lr: 9.411e-03, eta: 1 day, 14:08:42, time: 0.865, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4581, decode.acc_seg: 44.7792, loss: 1.4581
2021-08-13 13:54:07,316 - mmseg - INFO - Iter [10600/160000]	lr: 9.408e-03, eta: 1 day, 14:07:08, time: 0.850, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4311, decode.acc_seg: 46.5086, loss: 1.4311
2021-08-13 13:54:49,928 - mmseg - INFO - Iter [10650/160000]	lr: 9.405e-03, eta: 1 day, 14:05:36, time: 0.853, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4002, decode.acc_seg: 45.6263, loss: 1.4002
2021-08-13 13:55:31,956 - mmseg - INFO - Iter [10700/160000]	lr: 9.402e-03, eta: 1 day, 14:03:56, time: 0.840, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4834, decode.acc_seg: 44.8326, loss: 1.4834
2021-08-13 13:56:48,846 - mmseg - INFO - Iter [10750/160000]	lr: 9.399e-03, eta: 1 day, 14:10:20, time: 1.537, data_time: 0.676, memory: 4496, decode.loss_seg: 1.4156, decode.acc_seg: 45.2928, loss: 1.4156
2021-08-13 13:57:30,980 - mmseg - INFO - Iter [10800/160000]	lr: 9.397e-03, eta: 1 day, 14:08:40, time: 0.843, data_time: 0.011, memory: 4496, decode.loss_seg: 1.4003, decode.acc_seg: 45.7000, loss: 1.4003
2021-08-13 13:58:13,101 - mmseg - INFO - Iter [10850/160000]	lr: 9.394e-03, eta: 1 day, 14:07:00, time: 0.842, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4432, decode.acc_seg: 45.6590, loss: 1.4432
2021-08-13 13:58:55,455 - mmseg - INFO - Iter [10900/160000]	lr: 9.391e-03, eta: 1 day, 14:05:25, time: 0.847, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4502, decode.acc_seg: 45.6186, loss: 1.4502
2021-08-13 13:59:37,786 - mmseg - INFO - Iter [10950/160000]	lr: 9.388e-03, eta: 1 day, 14:03:49, time: 0.847, data_time: 0.009, memory: 4496, decode.loss_seg: 1.3929, decode.acc_seg: 45.1132, loss: 1.3929
2021-08-13 14:00:21,717 - mmseg - INFO - Exp name: fcn_litehr18-with-head_512x512_160k_ade20k.py
2021-08-13 14:00:21,717 - mmseg - INFO - Iter [11000/160000]	lr: 9.385e-03, eta: 1 day, 14:02:35, time: 0.878, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4508, decode.acc_seg: 45.9664, loss: 1.4508
2021-08-13 14:01:05,526 - mmseg - INFO - Iter [11050/160000]	lr: 9.383e-03, eta: 1 day, 14:01:20, time: 0.876, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4413, decode.acc_seg: 45.3581, loss: 1.4413
2021-08-13 14:01:48,031 - mmseg - INFO - Iter [11100/160000]	lr: 9.380e-03, eta: 1 day, 13:59:48, time: 0.850, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4296, decode.acc_seg: 45.2076, loss: 1.4296
2021-08-13 14:02:30,294 - mmseg - INFO - Iter [11150/160000]	lr: 9.377e-03, eta: 1 day, 13:58:13, time: 0.845, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4510, decode.acc_seg: 44.8667, loss: 1.4510
2021-08-13 14:03:12,896 - mmseg - INFO - Iter [11200/160000]	lr: 9.374e-03, eta: 1 day, 13:56:43, time: 0.852, data_time: 0.009, memory: 4496, decode.loss_seg: 1.4011, decode.acc_seg: 46.5160, loss: 1.4011
2021-08-13 14:03:54,806 - mmseg - INFO - Iter [11250/160000]	lr: 9.371e-03, eta: 1 day, 13:55:05, time: 0.838, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4498, decode.acc_seg: 44.7949, loss: 1.4498
2021-08-13 14:04:36,233 - mmseg - INFO - Iter [11300/160000]	lr: 9.369e-03, eta: 1 day, 13:53:20, time: 0.829, data_time: 0.009, memory: 4496, decode.loss_seg: 1.4747, decode.acc_seg: 44.9465, loss: 1.4747
2021-08-13 14:05:18,274 - mmseg - INFO - Iter [11350/160000]	lr: 9.366e-03, eta: 1 day, 13:51:44, time: 0.841, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4009, decode.acc_seg: 46.7334, loss: 1.4009
2021-08-13 14:06:35,571 - mmseg - INFO - Iter [11400/160000]	lr: 9.363e-03, eta: 1 day, 13:57:48, time: 1.545, data_time: 0.651, memory: 4496, decode.loss_seg: 1.3945, decode.acc_seg: 45.9498, loss: 1.3945
2021-08-13 14:07:18,030 - mmseg - INFO - Iter [11450/160000]	lr: 9.360e-03, eta: 1 day, 13:56:16, time: 0.850, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4410, decode.acc_seg: 46.0279, loss: 1.4410
2021-08-13 14:08:00,939 - mmseg - INFO - Iter [11500/160000]	lr: 9.357e-03, eta: 1 day, 13:54:51, time: 0.858, data_time: 0.009, memory: 4496, decode.loss_seg: 1.4455, decode.acc_seg: 45.2004, loss: 1.4455
2021-08-13 14:08:43,588 - mmseg - INFO - Iter [11550/160000]	lr: 9.354e-03, eta: 1 day, 13:53:22, time: 0.853, data_time: 0.009, memory: 4496, decode.loss_seg: 1.4238, decode.acc_seg: 46.6392, loss: 1.4238
2021-08-13 14:09:25,895 - mmseg - INFO - Iter [11600/160000]	lr: 9.352e-03, eta: 1 day, 13:51:50, time: 0.846, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4054, decode.acc_seg: 46.0379, loss: 1.4054
2021-08-13 14:10:08,365 - mmseg - INFO - Iter [11650/160000]	lr: 9.349e-03, eta: 1 day, 13:50:20, time: 0.850, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4177, decode.acc_seg: 46.2274, loss: 1.4177
2021-08-13 14:10:50,135 - mmseg - INFO - Iter [11700/160000]	lr: 9.346e-03, eta: 1 day, 13:48:41, time: 0.835, data_time: 0.009, memory: 4496, decode.loss_seg: 1.4021, decode.acc_seg: 46.6526, loss: 1.4021
2021-08-13 14:11:33,671 - mmseg - INFO - Iter [11750/160000]	lr: 9.343e-03, eta: 1 day, 13:47:26, time: 0.871, data_time: 0.009, memory: 4496, decode.loss_seg: 1.3886, decode.acc_seg: 46.3536, loss: 1.3886
2021-08-13 14:12:16,760 - mmseg - INFO - Iter [11800/160000]	lr: 9.340e-03, eta: 1 day, 13:46:05, time: 0.862, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4303, decode.acc_seg: 45.9720, loss: 1.4303
2021-08-13 14:12:58,586 - mmseg - INFO - Iter [11850/160000]	lr: 9.338e-03, eta: 1 day, 13:44:29, time: 0.837, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4439, decode.acc_seg: 45.0733, loss: 1.4439
2021-08-13 14:13:40,892 - mmseg - INFO - Iter [11900/160000]	lr: 9.335e-03, eta: 1 day, 13:42:58, time: 0.846, data_time: 0.009, memory: 4496, decode.loss_seg: 1.4017, decode.acc_seg: 45.2556, loss: 1.4017
2021-08-13 14:14:23,031 - mmseg - INFO - Iter [11950/160000]	lr: 9.332e-03, eta: 1 day, 13:41:27, time: 0.843, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4233, decode.acc_seg: 46.6145, loss: 1.4233
2021-08-13 14:15:41,210 - mmseg - INFO - Exp name: fcn_litehr18-with-head_512x512_160k_ade20k.py
2021-08-13 14:15:41,215 - mmseg - INFO - Iter [12000/160000]	lr: 9.329e-03, eta: 1 day, 13:47:20, time: 1.563, data_time: 0.721, memory: 4496, decode.loss_seg: 1.4260, decode.acc_seg: 45.7081, loss: 1.4260
2021-08-13 14:16:24,074 - mmseg - INFO - Iter [12050/160000]	lr: 9.326e-03, eta: 1 day, 13:45:56, time: 0.858, data_time: 0.011, memory: 4496, decode.loss_seg: 1.4121, decode.acc_seg: 45.7907, loss: 1.4121
2021-08-13 14:17:06,831 - mmseg - INFO - Iter [12100/160000]	lr: 9.324e-03, eta: 1 day, 13:44:31, time: 0.855, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3996, decode.acc_seg: 46.4536, loss: 1.3996
2021-08-13 14:17:49,160 - mmseg - INFO - Iter [12150/160000]	lr: 9.321e-03, eta: 1 day, 13:43:01, time: 0.847, data_time: 0.011, memory: 4496, decode.loss_seg: 1.4060, decode.acc_seg: 46.2977, loss: 1.4060
2021-08-13 14:18:31,739 - mmseg - INFO - Iter [12200/160000]	lr: 9.318e-03, eta: 1 day, 13:41:35, time: 0.851, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4234, decode.acc_seg: 45.6019, loss: 1.4234
2021-08-13 14:19:13,942 - mmseg - INFO - Iter [12250/160000]	lr: 9.315e-03, eta: 1 day, 13:40:04, time: 0.844, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4036, decode.acc_seg: 46.8443, loss: 1.4036
2021-08-13 14:19:56,035 - mmseg - INFO - Iter [12300/160000]	lr: 9.312e-03, eta: 1 day, 13:38:33, time: 0.842, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4047, decode.acc_seg: 46.1316, loss: 1.4047
2021-08-13 14:20:38,700 - mmseg - INFO - Iter [12350/160000]	lr: 9.310e-03, eta: 1 day, 13:37:09, time: 0.853, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4060, decode.acc_seg: 46.1535, loss: 1.4060
2021-08-13 14:21:20,634 - mmseg - INFO - Iter [12400/160000]	lr: 9.307e-03, eta: 1 day, 13:35:36, time: 0.838, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3698, decode.acc_seg: 46.2522, loss: 1.3698
2021-08-13 14:22:02,657 - mmseg - INFO - Iter [12450/160000]	lr: 9.304e-03, eta: 1 day, 13:34:05, time: 0.841, data_time: 0.011, memory: 4496, decode.loss_seg: 1.4161, decode.acc_seg: 45.9021, loss: 1.4161
2021-08-13 14:22:44,468 - mmseg - INFO - Iter [12500/160000]	lr: 9.301e-03, eta: 1 day, 13:32:32, time: 0.836, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4069, decode.acc_seg: 47.2086, loss: 1.4069
2021-08-13 14:23:27,523 - mmseg - INFO - Iter [12550/160000]	lr: 9.298e-03, eta: 1 day, 13:31:13, time: 0.861, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4008, decode.acc_seg: 45.5626, loss: 1.4008
2021-08-13 14:24:09,213 - mmseg - INFO - Iter [12600/160000]	lr: 9.296e-03, eta: 1 day, 13:29:39, time: 0.834, data_time: 0.011, memory: 4496, decode.loss_seg: 1.3819, decode.acc_seg: 47.0190, loss: 1.3819
2021-08-13 14:25:26,598 - mmseg - INFO - Iter [12650/160000]	lr: 9.293e-03, eta: 1 day, 13:35:02, time: 1.548, data_time: 0.717, memory: 4496, decode.loss_seg: 1.3696, decode.acc_seg: 46.5080, loss: 1.3696
2021-08-13 14:26:08,873 - mmseg - INFO - Iter [12700/160000]	lr: 9.290e-03, eta: 1 day, 13:33:34, time: 0.846, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3768, decode.acc_seg: 46.4991, loss: 1.3768
2021-08-13 14:26:51,057 - mmseg - INFO - Iter [12750/160000]	lr: 9.287e-03, eta: 1 day, 13:32:05, time: 0.843, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3580, decode.acc_seg: 47.0149, loss: 1.3580
2021-08-13 14:27:33,722 - mmseg - INFO - Iter [12800/160000]	lr: 9.284e-03, eta: 1 day, 13:30:42, time: 0.853, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3862, decode.acc_seg: 46.6005, loss: 1.3862
2021-08-13 14:28:16,092 - mmseg - INFO - Iter [12850/160000]	lr: 9.282e-03, eta: 1 day, 13:29:16, time: 0.848, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3869, decode.acc_seg: 46.2137, loss: 1.3869
2021-08-13 14:28:59,817 - mmseg - INFO - Iter [12900/160000]	lr: 9.279e-03, eta: 1 day, 13:28:05, time: 0.873, data_time: 0.009, memory: 4496, decode.loss_seg: 1.3790, decode.acc_seg: 46.2020, loss: 1.3790
2021-08-13 14:29:44,883 - mmseg - INFO - Iter [12950/160000]	lr: 9.276e-03, eta: 1 day, 13:27:11, time: 0.902, data_time: 0.011, memory: 4496, decode.loss_seg: 1.3845, decode.acc_seg: 47.1846, loss: 1.3845
2021-08-13 14:30:28,200 - mmseg - INFO - Exp name: fcn_litehr18-with-head_512x512_160k_ade20k.py
2021-08-13 14:30:28,201 - mmseg - INFO - Iter [13000/160000]	lr: 9.273e-03, eta: 1 day, 13:25:56, time: 0.866, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4087, decode.acc_seg: 47.1488, loss: 1.4087
2021-08-13 14:31:11,102 - mmseg - INFO - Iter [13050/160000]	lr: 9.270e-03, eta: 1 day, 13:24:38, time: 0.858, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3762, decode.acc_seg: 47.0105, loss: 1.3762
2021-08-13 14:31:54,364 - mmseg - INFO - Iter [13100/160000]	lr: 9.267e-03, eta: 1 day, 13:23:23, time: 0.865, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3468, decode.acc_seg: 45.7180, loss: 1.3468
2021-08-13 14:32:35,881 - mmseg - INFO - Iter [13150/160000]	lr: 9.265e-03, eta: 1 day, 13:21:50, time: 0.831, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4299, decode.acc_seg: 45.0180, loss: 1.4299
2021-08-13 14:33:19,602 - mmseg - INFO - Iter [13200/160000]	lr: 9.262e-03, eta: 1 day, 13:20:40, time: 0.874, data_time: 0.009, memory: 4496, decode.loss_seg: 1.3857, decode.acc_seg: 46.5067, loss: 1.3857
2021-08-13 14:34:01,228 - mmseg - INFO - Iter [13250/160000]	lr: 9.259e-03, eta: 1 day, 13:19:08, time: 0.833, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3917, decode.acc_seg: 46.0021, loss: 1.3917
2021-08-13 14:35:19,667 - mmseg - INFO - Iter [13300/160000]	lr: 9.256e-03, eta: 1 day, 13:24:23, time: 1.569, data_time: 0.663, memory: 4496, decode.loss_seg: 1.3874, decode.acc_seg: 46.0463, loss: 1.3874
2021-08-13 14:36:01,983 - mmseg - INFO - Iter [13350/160000]	lr: 9.253e-03, eta: 1 day, 13:22:58, time: 0.846, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3579, decode.acc_seg: 46.5285, loss: 1.3579
2021-08-13 14:36:44,566 - mmseg - INFO - Iter [13400/160000]	lr: 9.251e-03, eta: 1 day, 13:21:36, time: 0.851, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3436, decode.acc_seg: 47.6966, loss: 1.3436
2021-08-13 14:37:27,629 - mmseg - INFO - Iter [13450/160000]	lr: 9.248e-03, eta: 1 day, 13:20:19, time: 0.861, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4080, decode.acc_seg: 46.5022, loss: 1.4080
2021-08-13 14:38:10,825 - mmseg - INFO - Iter [13500/160000]	lr: 9.245e-03, eta: 1 day, 13:19:04, time: 0.864, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3944, decode.acc_seg: 46.1775, loss: 1.3944
2021-08-13 14:38:54,046 - mmseg - INFO - Iter [13550/160000]	lr: 9.242e-03, eta: 1 day, 13:17:50, time: 0.864, data_time: 0.009, memory: 4496, decode.loss_seg: 1.3462, decode.acc_seg: 46.4687, loss: 1.3462
2021-08-13 14:39:35,931 - mmseg - INFO - Iter [13600/160000]	lr: 9.239e-03, eta: 1 day, 13:16:22, time: 0.838, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4052, decode.acc_seg: 47.1040, loss: 1.4052
2021-08-13 14:40:18,385 - mmseg - INFO - Iter [13650/160000]	lr: 9.237e-03, eta: 1 day, 13:15:00, time: 0.849, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3521, decode.acc_seg: 47.6987, loss: 1.3521
2021-08-13 14:41:00,866 - mmseg - INFO - Iter [13700/160000]	lr: 9.234e-03, eta: 1 day, 13:13:38, time: 0.850, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4041, decode.acc_seg: 46.4305, loss: 1.4041
2021-08-13 14:41:42,991 - mmseg - INFO - Iter [13750/160000]	lr: 9.231e-03, eta: 1 day, 13:12:13, time: 0.842, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3884, decode.acc_seg: 46.7542, loss: 1.3884
2021-08-13 14:42:25,457 - mmseg - INFO - Iter [13800/160000]	lr: 9.228e-03, eta: 1 day, 13:10:52, time: 0.849, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3662, decode.acc_seg: 46.8425, loss: 1.3662
2021-08-13 14:43:07,374 - mmseg - INFO - Iter [13850/160000]	lr: 9.225e-03, eta: 1 day, 13:09:26, time: 0.838, data_time: 0.010, memory: 4496, decode.loss_seg: 1.4045, decode.acc_seg: 46.1532, loss: 1.4045
2021-08-13 14:44:25,603 - mmseg - INFO - Iter [13900/160000]	lr: 9.223e-03, eta: 1 day, 13:14:21, time: 1.564, data_time: 0.723, memory: 4496, decode.loss_seg: 1.4004, decode.acc_seg: 46.0395, loss: 1.4004
2021-08-13 14:45:08,165 - mmseg - INFO - Iter [13950/160000]	lr: 9.220e-03, eta: 1 day, 13:13:00, time: 0.851, data_time: 0.011, memory: 4496, decode.loss_seg: 1.3647, decode.acc_seg: 46.1632, loss: 1.3647
2021-08-13 14:45:51,379 - mmseg - INFO - Exp name: fcn_litehr18-with-head_512x512_160k_ade20k.py
2021-08-13 14:45:51,380 - mmseg - INFO - Iter [14000/160000]	lr: 9.217e-03, eta: 1 day, 13:11:47, time: 0.865, data_time: 0.011, memory: 4496, decode.loss_seg: 1.4106, decode.acc_seg: 46.7094, loss: 1.4106
2021-08-13 14:46:33,209 - mmseg - INFO - Iter [14050/160000]	lr: 9.214e-03, eta: 1 day, 13:10:20, time: 0.837, data_time: 0.011, memory: 4496, decode.loss_seg: 1.3237, decode.acc_seg: 47.0991, loss: 1.3237
2021-08-13 14:47:16,122 - mmseg - INFO - Iter [14100/160000]	lr: 9.211e-03, eta: 1 day, 13:09:03, time: 0.858, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3943, decode.acc_seg: 45.7484, loss: 1.3943
2021-08-13 14:47:59,417 - mmseg - INFO - Iter [14150/160000]	lr: 9.208e-03, eta: 1 day, 13:07:51, time: 0.866, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3745, decode.acc_seg: 46.5204, loss: 1.3745
2021-08-13 14:48:41,420 - mmseg - INFO - Iter [14200/160000]	lr: 9.206e-03, eta: 1 day, 13:06:26, time: 0.840, data_time: 0.011, memory: 4496, decode.loss_seg: 1.3679, decode.acc_seg: 47.4450, loss: 1.3679
2021-08-13 14:49:25,114 - mmseg - INFO - Iter [14250/160000]	lr: 9.203e-03, eta: 1 day, 13:05:19, time: 0.873, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3702, decode.acc_seg: 46.9593, loss: 1.3702
2021-08-13 14:50:07,540 - mmseg - INFO - Iter [14300/160000]	lr: 9.200e-03, eta: 1 day, 13:03:58, time: 0.849, data_time: 0.011, memory: 4496, decode.loss_seg: 1.3640, decode.acc_seg: 46.0294, loss: 1.3640
2021-08-13 14:50:50,649 - mmseg - INFO - Iter [14350/160000]	lr: 9.197e-03, eta: 1 day, 13:02:45, time: 0.862, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3401, decode.acc_seg: 47.7235, loss: 1.3401
2021-08-13 14:51:33,724 - mmseg - INFO - Iter [14400/160000]	lr: 9.194e-03, eta: 1 day, 13:01:32, time: 0.861, data_time: 0.011, memory: 4496, decode.loss_seg: 1.3858, decode.acc_seg: 47.0783, loss: 1.3858
2021-08-13 14:52:16,914 - mmseg - INFO - Iter [14450/160000]	lr: 9.192e-03, eta: 1 day, 13:00:20, time: 0.865, data_time: 0.011, memory: 4496, decode.loss_seg: 1.3500, decode.acc_seg: 47.2848, loss: 1.3500
2021-08-13 14:52:58,934 - mmseg - INFO - Iter [14500/160000]	lr: 9.189e-03, eta: 1 day, 12:58:57, time: 0.840, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3302, decode.acc_seg: 48.3464, loss: 1.3302
2021-08-13 14:54:17,154 - mmseg - INFO - Iter [14550/160000]	lr: 9.186e-03, eta: 1 day, 13:03:36, time: 1.564, data_time: 0.724, memory: 4496, decode.loss_seg: 1.3734, decode.acc_seg: 46.0073, loss: 1.3734
2021-08-13 14:55:00,008 - mmseg - INFO - Iter [14600/160000]	lr: 9.183e-03, eta: 1 day, 13:02:20, time: 0.858, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3686, decode.acc_seg: 46.9254, loss: 1.3686
2021-08-13 14:55:42,256 - mmseg - INFO - Iter [14650/160000]	lr: 9.180e-03, eta: 1 day, 13:00:58, time: 0.845, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3541, decode.acc_seg: 46.9414, loss: 1.3541
2021-08-13 14:56:25,504 - mmseg - INFO - Iter [14700/160000]	lr: 9.178e-03, eta: 1 day, 12:59:47, time: 0.864, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3383, decode.acc_seg: 46.9331, loss: 1.3383
2021-08-13 14:57:10,329 - mmseg - INFO - Iter [14750/160000]	lr: 9.175e-03, eta: 1 day, 12:58:51, time: 0.897, data_time: 0.011, memory: 4496, decode.loss_seg: 1.3403, decode.acc_seg: 47.1603, loss: 1.3403
2021-08-13 14:57:55,434 - mmseg - INFO - Iter [14800/160000]	lr: 9.172e-03, eta: 1 day, 12:57:58, time: 0.902, data_time: 0.011, memory: 4496, decode.loss_seg: 1.3942, decode.acc_seg: 47.0931, loss: 1.3942
2021-08-13 14:58:38,179 - mmseg - INFO - Iter [14850/160000]	lr: 9.169e-03, eta: 1 day, 12:56:42, time: 0.856, data_time: 0.011, memory: 4496, decode.loss_seg: 1.3518, decode.acc_seg: 47.5090, loss: 1.3518
2021-08-13 14:59:20,221 - mmseg - INFO - Iter [14900/160000]	lr: 9.166e-03, eta: 1 day, 12:55:20, time: 0.841, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3555, decode.acc_seg: 47.2029, loss: 1.3555
2021-08-13 15:00:02,857 - mmseg - INFO - Iter [14950/160000]	lr: 9.163e-03, eta: 1 day, 12:54:03, time: 0.853, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3389, decode.acc_seg: 47.4363, loss: 1.3389
2021-08-13 15:00:45,678 - mmseg - INFO - Exp name: fcn_litehr18-with-head_512x512_160k_ade20k.py
2021-08-13 15:00:45,678 - mmseg - INFO - Iter [15000/160000]	lr: 9.161e-03, eta: 1 day, 12:52:49, time: 0.857, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3695, decode.acc_seg: 46.6565, loss: 1.3695
2021-08-13 15:01:28,546 - mmseg - INFO - Iter [15050/160000]	lr: 9.158e-03, eta: 1 day, 12:51:35, time: 0.857, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3137, decode.acc_seg: 47.7247, loss: 1.3137
2021-08-13 15:02:10,246 - mmseg - INFO - Iter [15100/160000]	lr: 9.155e-03, eta: 1 day, 12:50:10, time: 0.835, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3308, decode.acc_seg: 47.4356, loss: 1.3308
2021-08-13 15:03:28,054 - mmseg - INFO - Iter [15150/160000]	lr: 9.152e-03, eta: 1 day, 12:54:31, time: 1.555, data_time: 0.719, memory: 4496, decode.loss_seg: 1.3189, decode.acc_seg: 47.8840, loss: 1.3189
2021-08-13 15:04:10,258 - mmseg - INFO - Iter [15200/160000]	lr: 9.149e-03, eta: 1 day, 12:53:10, time: 0.844, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3425, decode.acc_seg: 46.5467, loss: 1.3425
2021-08-13 15:04:52,288 - mmseg - INFO - Iter [15250/160000]	lr: 9.147e-03, eta: 1 day, 12:51:48, time: 0.841, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3568, decode.acc_seg: 47.1304, loss: 1.3568
2021-08-13 15:05:34,572 - mmseg - INFO - Iter [15300/160000]	lr: 9.144e-03, eta: 1 day, 12:50:28, time: 0.845, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3753, decode.acc_seg: 47.9929, loss: 1.3753
2021-08-13 15:06:17,004 - mmseg - INFO - Iter [15350/160000]	lr: 9.141e-03, eta: 1 day, 12:49:10, time: 0.849, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3476, decode.acc_seg: 46.9175, loss: 1.3476
2021-08-13 15:07:00,505 - mmseg - INFO - Iter [15400/160000]	lr: 9.138e-03, eta: 1 day, 12:48:03, time: 0.870, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3738, decode.acc_seg: 47.4495, loss: 1.3738
2021-08-13 15:07:43,357 - mmseg - INFO - Iter [15450/160000]	lr: 9.135e-03, eta: 1 day, 12:46:50, time: 0.858, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3653, decode.acc_seg: 47.9887, loss: 1.3653
2021-08-13 15:08:26,060 - mmseg - INFO - Iter [15500/160000]	lr: 9.133e-03, eta: 1 day, 12:45:35, time: 0.854, data_time: 0.009, memory: 4496, decode.loss_seg: 1.3349, decode.acc_seg: 46.5817, loss: 1.3349
2021-08-13 15:09:07,549 - mmseg - INFO - Iter [15550/160000]	lr: 9.130e-03, eta: 1 day, 12:44:09, time: 0.829, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3423, decode.acc_seg: 46.4540, loss: 1.3423
2021-08-13 15:09:50,375 - mmseg - INFO - Iter [15600/160000]	lr: 9.127e-03, eta: 1 day, 12:42:56, time: 0.857, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3478, decode.acc_seg: 46.5233, loss: 1.3478
2021-08-13 15:10:33,480 - mmseg - INFO - Iter [15650/160000]	lr: 9.124e-03, eta: 1 day, 12:41:45, time: 0.862, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3435, decode.acc_seg: 47.4334, loss: 1.3435
2021-08-13 15:11:16,829 - mmseg - INFO - Iter [15700/160000]	lr: 9.121e-03, eta: 1 day, 12:40:38, time: 0.867, data_time: 0.011, memory: 4496, decode.loss_seg: 1.3480, decode.acc_seg: 47.8879, loss: 1.3480
2021-08-13 15:12:00,950 - mmseg - INFO - Iter [15750/160000]	lr: 9.118e-03, eta: 1 day, 12:39:37, time: 0.882, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3078, decode.acc_seg: 46.9986, loss: 1.3078
2021-08-13 15:13:19,041 - mmseg - INFO - Iter [15800/160000]	lr: 9.116e-03, eta: 1 day, 12:43:46, time: 1.561, data_time: 0.716, memory: 4496, decode.loss_seg: 1.3409, decode.acc_seg: 47.7085, loss: 1.3409
2021-08-13 15:14:01,769 - mmseg - INFO - Iter [15850/160000]	lr: 9.113e-03, eta: 1 day, 12:42:32, time: 0.855, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3105, decode.acc_seg: 46.9232, loss: 1.3105
2021-08-13 15:14:43,772 - mmseg - INFO - Iter [15900/160000]	lr: 9.110e-03, eta: 1 day, 12:41:12, time: 0.840, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3233, decode.acc_seg: 48.7790, loss: 1.3233
2021-08-13 15:15:26,931 - mmseg - INFO - Iter [15950/160000]	lr: 9.107e-03, eta: 1 day, 12:40:02, time: 0.863, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3014, decode.acc_seg: 47.3112, loss: 1.3014
2021-08-13 15:16:12,307 - mmseg - INFO - Saving checkpoint at 16000 iterations
2021-08-13 15:16:12,553 - mmseg - INFO - Exp name: fcn_litehr18-with-head_512x512_160k_ade20k.py
2021-08-13 15:16:12,555 - mmseg - INFO - Iter [16000/160000]	lr: 9.104e-03, eta: 1 day, 12:39:14, time: 0.913, data_time: 0.010, memory: 4496, decode.loss_seg: 1.3759, decode.acc_seg: 47.0626, loss: 1.3759
